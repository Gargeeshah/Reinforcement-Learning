# -*- coding: utf-8 -*-
"""RL-Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P73BD6ifE_5gIHvJ8cIKu5hRe683qCDw

# **REINFORCE with Baseline algorithm**

To Run:
# python3 rwb-cartpole.py
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import gym
import numpy as np
import matplotlib.pyplot as plt

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 6, 5)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.conv3 = nn.Conv2d(8, 32, 5)
        self.fc1 = nn.Linear(32 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))
        x = F.max_pool2d(F.relu(self.conv2(x)), 2)
        x = F.max_pool2d(F.relu(self.conv3(x)), 2)
        x = x.view(-1, self.num_flat_features(x))
        x = F.relu(self.fc1(x))

class PolicyNetwork(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(PolicyNetwork, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, output_size)
        # self.softmax = nn.Softmax(dim=-1)
        # self.relu = nn.ReLU()
        # self.dropout = nn.Dropout(p=0.5)
        # self.batchnorm = nn.BatchNorm1d(hidden_size)
        # self.init_weights()

    # def init_weights(self):
    #     for m in self.modules():
    #         if isinstance(m, nn.Linear):
    #             nn.init.xavier_uniform_(m.weight)
    #             nn.init.constant_(m.bias, 0)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = torch.softmax(self.fc3(x), dim=-1)
        # x = self.relu(x)
        # x = self.dropout(x)
        # x = self.batchnorm(x)
        # x = F.relu(x)
        return x

class StateValueNetwork(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(StateValueNetwork, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, 1)
        # self.softmax = nn.Softmax(dim=-1)
        # self.dropout = nn.Dropout(p=0.5)
        # self.batchnorm = nn.BatchNorm1d(hidden_size)
        # self.init_weights()

    # def init_weights(self):
    #     for m in self.modules():
    #         if isinstance(m, nn.Linear):
    #             nn.init.xavier_uniform_(m.weight)
    #             nn.init.constant_(m.bias, 0)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = torch.relu(self.fc3(x))
        # x = self.dropout(x)
        # x = self.batchnorm(x)
        # x = F.relu(x)
        # x = self.softmax(x)
        return x

def compute_policy_gradient_loss(action_probabilities, advantages):
    log_probabilities = torch.log(torch.stack(action_probabilities))
    return -torch.sum(log_probabilities * advantages)

def reinforce_with_baseline_cartpole(policy_net, state_value_net, policy_optimizer, state_value_optimizer, env, num_episodes, gamma):

    all_episode_rewards = []
    all_episode_steps = []

    for episode in range(num_episodes):
        state = env.reset()
        state = torch.tensor(state, dtype=torch.float32)
        done = False
        episode_states = []
        episode_actions = []
        episode_rewards = []
        episode_action_probs = []
        episode_values = []
        episode_steps = 0

        while not done:
            action_probs = policy_net(state)
            action = torch.multinomial(action_probs, 1).item()
            # action_probs = action_probs.detach().numpy()[0]
            # next_state, reward, done, extra_value = env.step(action)
            step_result = env.step(action)

            reward = step_result[1]
            done = step_result[2]
            timeout = step_result[3]

            next_state = step_result[0]
            next_state = torch.tensor(next_state, dtype=torch.float32)

            episode_states.append(state)
            episode_action_probs.append(action_probs[action])
            episode_actions.append(action)
            episode_rewards.append(reward)

            state = next_state
            episode_steps += 1

        # episode_states = torch.stack(episode_states)

        returns_calculation = []
        G = 0
        episode_rewards_array = episode_rewards[::-1]
        for r in (range(len(episode_rewards_array))):
            reward = episode_rewards_array[r]
            G = reward + gamma * G
            returns_calculation.append(G)
        # print("Episode Rewards (reversed)", episode_rewards)
        returns_calculation = returns_calculation[::-1]
        # print("Calculated Returns", returns_calculation)
        returns_calculation = torch.tensor(returns_calculation, dtype=torch.float32)
        # print("Episode Action Probs", episode_action_probs)

        stacked_episode_states = torch.stack(episode_states)
        stacked_episode_values = state_value_net(stacked_episode_states)
        stacked_episode_val = stacked_episode_values.squeeze(1)
        # print("Stacked Episode Values", stacked_episode_values)
        # stacked_ep_val = stacked_episode_values.detach().numpy()
        stacked_ep_val = stacked_episode_values.detach().numpy()
        stacked_ep_val_flatten = stacked_ep_val.flatten()
        # stacked_ep_val_tensor = torch.tensor(stacked_ep_val_flatten, dtype=torch.float32)
        stacked_ep_val_tensor = torch.tensor(stacked_ep_val_flatten, dtype=torch.float32)
        calculated_adv = returns_calculation - stacked_ep_val_tensor

        policy_loss = []
        for i in range(len(episode_action_probs)):
            action_prob = episode_action_probs[i]
            calculated_adv_value = calculated_adv[i]

            prob_detached = action_prob.detach()
            adv_detached = calculated_adv_value.detach()
            log_prob = torch.log(prob_detached)

            action_loss = -log_prob * adv_detached
            policy_loss.append(action_loss)
            # policy_loss.append(-torch.log(action_prob.detach()) * calculated_adv.detach())
        policy_loss = torch.stack(policy_loss).sum()

        policy_optimizer.zero_grad()
        state_values_calculation = []
        for state in episode_states:
            state_value_net.eval()
            state_values_calculation.append(state_value_net(state))
        state_values_calculation = torch.cat([state_value_net(state) for state in episode_states])
        advantages = returns_calculation - state_values_calculation.detach()
        policy_optimizer_loss = compute_policy_gradient_loss(episode_action_probs, advantages)
        # policy_optimizer_loss = -torch.stack(episode_action_probs).sum() * returns_calculation.mean()
        # log_action_probs = torch.log(torch.stack(episode_action_probs))
        # policy_optimizer_loss = -(log_action_probs * advantages).sum()
        # policy_optimizer_loss = -torch.sum(torch.log(torch.stack(episode_action_probs)) * advantages)
        # policy_optimizer_loss = -torch.stack(episode_action_probs).sum() * returns_calculation.mean()
        policy_optimizer_loss.backward()
        policy_optimizer.step()

        state_value_optimizer.zero_grad()
        # state_value_loss = F.mse_loss(state_value_net(state), returns_calculation)
        episode_states_stack = torch.stack(episode_states)
        state_values_calculation = state_value_net(episode_states_stack)
        mse_loss_function = nn.MSELoss()
        state_value_loss = mse_loss_function(state_values_calculation, returns_calculation)
        state_value_loss.backward()
        state_value_optimizer.step()

        total_reward = sum(episode_rewards)
        if (episode + 1) % 50 == 0:
            print(f"Episode: {episode}, Reward: {total_reward}")
            # print(f"Policy Loss: {policy_optimizer_loss.item()}, State Value Loss: {state_value_loss.item()}")

        all_episode_rewards.append(total_reward)
        all_episode_steps.append(episode_steps)

    return policy_net, state_value_net, all_episode_rewards, all_episode_steps

# env = gym.make('MountainCar-v0')
env = gym.make('CartPole-v1')
# env = gym.make('Acrobot-v1')
# env = gym.make('LunarLander-v2')
env.reset()

input_size = env.observation_space.shape[0]
hidden_size = 128
output_size = env.action_space.n

policy_net = PolicyNetwork(input_size, hidden_size, output_size)
state_value_net = StateValueNetwork(input_size, hidden_size)

policy_net.eval()
state_value_net.eval()

#Parameters lr
policy_optimizer = torch.optim.Adam(policy_net.parameters(), lr=0.001)
state_value_optimizer = torch.optim.Adam(state_value_net.parameters(), lr=0.001)

print(policy_net)
print(state_value_net)

# env = gym.make('MountainCar-v0')
env = gym.make('CartPole-v1')
# env = gym.make('Acrobot-v1')
# env = gym.make('LunarLander-v2')
env.reset()

input_size = env.observation_space.shape[0]
hidden_size = 128
output_size = env.action_space.n

policy_net = PolicyNetwork(input_size, hidden_size, output_size)
state_value_net = StateValueNetwork(input_size, hidden_size)

policy_net.eval()
state_value_net.eval()

#Parameters lr
policy_optimizer = torch.optim.Adam(policy_net.parameters(), lr=0.001)
state_value_optimizer = torch.optim.Adam(state_value_net.parameters(), lr=0.001)

print(policy_net)
print(state_value_net)

"""# **CartPole-v1**"""

env_name = 'CartPole-v1'
# env.make(env_name)

all_episode_rewards = []
all_episode_steps = []

gamma = 0.925
alpha_theta = 0.01
alpha_w = 0.01

RUNS = 10
num_episodes = 1000

for run in range(RUNS):
    print(f"Run: {run}")
    env = gym.make(env_name)
    # env.reset()

    input_size = env.observation_space.shape[0]
    hidden_size = 128
    output_size = env.action_space.n

    policy_net = PolicyNetwork(input_size, hidden_size, output_size)
    state_value_net = StateValueNetwork(input_size, hidden_size)

    policy_optimizer = torch.optim.Adam(policy_net.parameters(), lr=alpha_theta)
    state_value_optimizer = torch.optim.Adam(state_value_net.parameters(), lr=alpha_w)

    policy_net, state_value_net, episode_rewards, episode_steps = reinforce_with_baseline_cartpole(policy_net, state_value_net, policy_optimizer, state_value_optimizer, env, num_episodes, gamma)
    all_episode_rewards.append(episode_rewards)
    all_episode_steps.append(episode_steps)

all_episode_rewards = np.array(all_episode_rewards)
all_episode_steps = np.array(all_episode_steps)
# print(all_episode_rewards)
# print(all_episode_steps)

all_episode_rewards_mean = all_episode_rewards.mean(axis=0)
all_episode_steps_mean = all_episode_steps.mean(axis=0)

all_episode_rewards_std = all_episode_rewards.std(axis=0)
all_episode_steps_std = all_episode_steps.std(axis=0)

plt.figure(figsize=(10, 6))
plt.plot(np.arange(1, num_episodes + 1), all_episode_rewards_mean, label='Mean Reward', color='blue')
plt.fill_between(np.arange(1, num_episodes + 1),
                 all_episode_rewards_mean - all_episode_rewards_std,
                 all_episode_rewards_mean + all_episode_rewards_std,
                 color='lightblue', alpha=0.6, label='Standard Deviation')
plt.title('Average Episode Reward vs. Number of Episodes in Cartpole', fontsize=14)
plt.xlabel('Number of Episodes', fontsize=12)
plt.ylabel('Average Reward per Episode', fontsize=12)
plt.legend(fontsize=10)
plt.show()


plt.figure(figsize=(10, 6))
plt.plot(np.arange(1, num_episodes + 1), all_episode_steps_mean, label='Mean Steps', color='green')
plt.fill_between(np.arange(1, num_episodes + 1),
                 all_episode_steps_mean - all_episode_steps_std,
                 all_episode_steps_mean + all_episode_steps_std,
                 color='lightgreen', alpha=0.5, label='Standard Deviation')
plt.title('Average Episode Steps vs. Number of Episodes in Cartpole', fontsize=14)
plt.xlabel('Number of Episodes', fontsize=12)
plt.ylabel('Average Steps', fontsize=12)
plt.legend(fontsize=10)
plt.show()

total_actions_taken = np.cumsum(all_episode_steps_mean)
plt.plot(total_actions_taken, np.arange(1, num_episodes + 1), label='Learning Curve', color='orange')
# plt.fill_between(np.arange(1, num_episodes + 1),
#                  all_episode_steps_mean - all_episode_steps_std,
#                  all_episode_steps_mean + all_episode_steps_std,
#                  color='lightcoral', alpha=0.5, label='Standard Deviation')
plt.title('Cumulative Actions Taken vs. Number of Episodes in Cartpole', fontsize=14)
plt.xlabel('Cumulative Actions Taken', fontsize=12)
plt.ylabel('Number of Episodes', fontsize=12)
plt.legend(fontsize=10)
plt.show()


# # -*- coding: utf-8 -*-
# """RL-Project.ipynb

# Automatically generated by Colab.

# Original file is located at
#     https://colab.research.google.com/drive/1P73BD6ifE_5gIHvJ8cIKu5hRe683qCDw

# # **REINFORCE with Baseline algorithm**
# """

# import torch
# import torch.nn as nn
# import torch.nn.functional as F
# import gym
# import numpy as np
# import matplotlib.pyplot as plt

# class Net(nn.Module):
#     def __init__(self):
#         super(Net, self).__init__()
#         self.conv1 = nn.Conv2d(1, 6, 5)
#         self.conv2 = nn.Conv2d(6, 16, 5)
#         self.conv3 = nn.Conv2d(8, 32, 5)
#         self.fc1 = nn.Linear(32 * 5 * 5, 120)
#         self.fc2 = nn.Linear(120, 84)
#         self.fc3 = nn.Linear(84, 10)

#     def forward(self, x):
#         x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))
#         x = F.max_pool2d(F.relu(self.conv2(x)), 2)
#         x = F.max_pool2d(F.relu(self.conv3(x)), 2)
#         x = x.view(-1, self.num_flat_features(x))
#         x = F.relu(self.fc1(x))

# class PolicyNetwork(nn.Module):
#     def __init__(self, input_size, hidden_size, output_size):
#         super(PolicyNetwork, self).__init__()
#         self.fc1 = nn.Linear(input_size, hidden_size)
#         self.fc2 = nn.Linear(hidden_size, hidden_size)
#         self.fc3 = nn.Linear(hidden_size, output_size)
#         # self.softmax = nn.Softmax(dim=-1)
#         # self.relu = nn.ReLU()
#         # self.dropout = nn.Dropout(p=0.5)
#         # self.batchnorm = nn.BatchNorm1d(hidden_size)
#         # self.init_weights()

#     # def init_weights(self):
#     #     for m in self.modules():
#     #         if isinstance(m, nn.Linear):
#     #             nn.init.xavier_uniform_(m.weight)
#     #             nn.init.constant_(m.bias, 0)

#     def forward(self, x):
#         x = torch.relu(self.fc1(x))
#         x = torch.relu(self.fc2(x))
#         x = torch.softmax(self.fc3(x), dim=-1)
#         # x = self.relu(x)
#         # x = self.dropout(x)
#         # x = self.batchnorm(x)
#         # x = F.relu(x)
#         return x

# class StateValueNetwork(nn.Module):
#     def __init__(self, input_size, hidden_size):
#         super(StateValueNetwork, self).__init__()
#         self.fc1 = nn.Linear(input_size, hidden_size)
#         self.fc2 = nn.Linear(hidden_size, hidden_size)
#         self.fc3 = nn.Linear(hidden_size, 1)
#         # self.softmax = nn.Softmax(dim=-1)
#         # self.dropout = nn.Dropout(p=0.5)
#         # self.batchnorm = nn.BatchNorm1d(hidden_size)
#         # self.init_weights()

#     # def init_weights(self):
#     #     for m in self.modules():
#     #         if isinstance(m, nn.Linear):
#     #             nn.init.xavier_uniform_(m.weight)
#     #             nn.init.constant_(m.bias, 0)

#     def forward(self, x):
#         x = torch.relu(self.fc1(x))
#         x = torch.relu(self.fc2(x))
#         x = torch.relu(self.fc3(x))
#         # x = self.dropout(x)
#         # x = self.batchnorm(x)
#         # x = F.relu(x)
#         # x = self.softmax(x)
#         return x

# def compute_policy_gradient_loss(action_probabilities, advantages):
#     log_probabilities = torch.log(torch.stack(action_probabilities))
#     return -torch.sum(log_probabilities * advantages)

# def reinforce_with_baseline_cartpole(policy_net, state_value_net, policy_optimizer, state_value_optimizer, env, num_episodes, gamma):

#     all_episode_rewards = []
#     all_episode_steps = []

#     for episode in range(num_episodes):
#         state = env.reset()
#         state = torch.tensor(state, dtype=torch.float32)
#         done = False
#         episode_states = []
#         episode_actions = []
#         episode_rewards = []
#         episode_action_probs = []
#         episode_values = []
#         episode_steps = 0

#         while not done:
#             action_probs = policy_net(state)
#             action = torch.multinomial(action_probs, 1).item()
#             # action_probs = action_probs.detach().numpy()[0]
#             # next_state, reward, done, extra_value = env.step(action)
#             step_result = env.step(action)

#             reward = step_result[1]
#             done = step_result[2]
#             extra_value = step_result[3]

#             next_state = step_result[0]
#             next_state = torch.tensor(next_state, dtype=torch.float32)

#             episode_states.append(state)
#             episode_action_probs.append(action_probs[action])
#             episode_actions.append(action)
#             episode_rewards.append(reward)

#             state = next_state
#             episode_steps += 1

#         # episode_states = torch.stack(episode_states)

#         returns_calculation = []
#         G = 0
#         episode_rewards_array = episode_rewards[::-1]
#         for r in (range(len(episode_rewards_array))):
#             reward = episode_rewards_array[r]
#             G = reward + gamma * G
#             returns_calculation.append(G)
#         # print("Episode Rewards (reversed)", episode_rewards)
#         returns_calculation = returns_calculation[::-1]
#         # print("Calculated Returns", returns_calculation)
#         returns_calculation = torch.tensor(returns_calculation, dtype=torch.float32)
#         # print("Episode Action Probs", episode_action_probs)

#         stacked_episode_states = torch.stack(episode_states)
#         stacked_episode_values = state_value_net(stacked_episode_states)
#         stacked_episode_val = stacked_episode_values.squeeze(1)
#         # print("Stacked Episode Values", stacked_episode_values)
#         # stacked_ep_val = stacked_episode_values.detach().numpy()
#         stacked_ep_val = stacked_episode_values.detach().numpy()
#         stacked_ep_val_flatten = stacked_ep_val.flatten()
#         # stacked_ep_val_tensor = torch.tensor(stacked_ep_val_flatten, dtype=torch.float32)
#         stacked_ep_val_tensor = torch.tensor(stacked_ep_val_flatten, dtype=torch.float32)
#         calculated_adv = returns_calculation - stacked_ep_val_tensor

#         policy_loss = []
#         for i in range(len(episode_action_probs)):
#             action_prob = episode_action_probs[i]
#             calculated_adv_value = calculated_adv[i]

#             prob_detached = action_prob.detach()
#             adv_detached = calculated_adv_value.detach()
#             log_prob = torch.log(prob_detached)

#             action_loss = -log_prob * adv_detached
#             policy_loss.append(action_loss)
#             # policy_loss.append(-torch.log(action_prob.detach()) * calculated_adv.detach())
#         policy_loss = torch.stack(policy_loss).sum()

#         policy_optimizer.zero_grad()
#         state_values_calculation = []
#         for state in episode_states:
#             state_value_net.eval()
#             state_values_calculation.append(state_value_net(state))
#         state_values_calculation = torch.cat([state_value_net(state) for state in episode_states])
#         advantages = returns_calculation - state_values_calculation.detach()
#         policy_optimizer_loss = compute_policy_gradient_loss(episode_action_probs, advantages)
#         # policy_optimizer_loss = -torch.stack(episode_action_probs).sum() * returns_calculation.mean()
#         # log_action_probs = torch.log(torch.stack(episode_action_probs))
#         # policy_optimizer_loss = -(log_action_probs * advantages).sum()
#         # policy_optimizer_loss = -torch.sum(torch.log(torch.stack(episode_action_probs)) * advantages)
#         # policy_optimizer_loss = -torch.stack(episode_action_probs).sum() * returns_calculation.mean()
#         policy_optimizer_loss.backward()
#         policy_optimizer.step()

#         state_value_optimizer.zero_grad()
#         # state_value_loss = F.mse_loss(state_value_net(state), returns_calculation)
#         episode_states_stack = torch.stack(episode_states)
#         state_values_calculation = state_value_net(episode_states_stack)
#         mse_loss_function = nn.MSELoss()
#         state_value_loss = mse_loss_function(state_values_calculation, returns_calculation)
#         state_value_loss.backward()
#         state_value_optimizer.step()

#         total_reward = sum(episode_rewards)
#         if (episode + 1) % 50 == 0:
#             print(f"Episode: {episode}, Reward: {total_reward}")
#             # print(f"Policy Loss: {policy_optimizer_loss.item()}, State Value Loss: {state_value_loss.item()}")

#         all_episode_rewards.append(total_reward)
#         all_episode_steps.append(episode_steps)

#     return policy_net, state_value_net, all_episode_rewards, all_episode_steps

# # env = gym.make('MountainCar-v0')
# env = gym.make('CartPole-v1')
# # env = gym.make('Acrobot-v1')
# # env = gym.make('LunarLander-v2')
# env.reset()

# input_size = env.observation_space.shape[0]
# hidden_size = 128
# output_size = env.action_space.n

# policy_net = PolicyNetwork(input_size, hidden_size, output_size)
# state_value_net = StateValueNetwork(input_size, hidden_size)

# policy_net.eval()
# state_value_net.eval()

# #Parameters lr
# policy_optimizer = torch.optim.Adam(policy_net.parameters(), lr=0.001)
# state_value_optimizer = torch.optim.Adam(state_value_net.parameters(), lr=0.001)

# print(policy_net)
# print(state_value_net)

# # env = gym.make('MountainCar-v0')
# env = gym.make('CartPole-v1')
# # env = gym.make('Acrobot-v1')
# # env = gym.make('LunarLander-v2')
# env.reset()

# input_size = env.observation_space.shape[0]
# hidden_size = 128
# output_size = env.action_space.n

# policy_net = PolicyNetwork(input_size, hidden_size, output_size)
# state_value_net = StateValueNetwork(input_size, hidden_size)

# policy_net.eval()
# state_value_net.eval()

# #Parameters lr
# policy_optimizer = torch.optim.Adam(policy_net.parameters(), lr=0.001)
# state_value_optimizer = torch.optim.Adam(state_value_net.parameters(), lr=0.001)

# print(policy_net)
# print(state_value_net)

# """# **CartPole-v1**"""

# env_name = 'CartPole-v1'
# # env.make(env_name)

# all_episode_rewards = []
# all_episode_steps = []

# gamma = 0.925
# alpha_theta = 0.01
# alpha_w = 0.01

# RUNS = 10
# num_episodes = 1000

# for run in range(RUNS):
#     print(f"Run: {run}")
#     env = gym.make(env_name)
#     # env.reset()

#     input_size = env.observation_space.shape[0]
#     hidden_size = 128
#     output_size = env.action_space.n

#     policy_net = PolicyNetwork(input_size, hidden_size, output_size)
#     state_value_net = StateValueNetwork(input_size, hidden_size)

#     policy_optimizer = torch.optim.Adam(policy_net.parameters(), lr=alpha_theta)
#     state_value_optimizer = torch.optim.Adam(state_value_net.parameters(), lr=alpha_w)

#     policy_net, state_value_net, episode_rewards, episode_steps = reinforce_with_baseline_cartpole(policy_net, state_value_net, policy_optimizer, state_value_optimizer, env, num_episodes, gamma)
#     all_episode_rewards.append(episode_rewards)
#     all_episode_steps.append(episode_steps)

# all_episode_rewards = np.array(all_episode_rewards)
# all_episode_steps = np.array(all_episode_steps)
# # print(all_episode_rewards)
# # print(all_episode_steps)

# all_episode_rewards_mean = all_episode_rewards.mean(axis=0)
# all_episode_steps_mean = all_episode_steps.mean(axis=0)

# all_episode_rewards_std = all_episode_rewards.std(axis=0)
# all_episode_steps_std = all_episode_steps.std(axis=0)

# plt.figure(figsize=(10, 6))
# plt.plot(np.arange(1, num_episodes + 1), all_episode_rewards_mean, label='Mean Reward', color='blue')
# plt.fill_between(np.arange(1, num_episodes + 1),
#                  all_episode_rewards_mean - all_episode_rewards_std,
#                  all_episode_rewards_mean + all_episode_rewards_std,
#                  color='lightblue', alpha=0.6, label='Standard Deviation')
# plt.title('Average Episode Reward vs. Number of Episodes in Cartpole', fontsize=14)
# plt.xlabel('Number of Episodes', fontsize=12)
# plt.ylabel('Average Reward per Episode', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()


# plt.figure(figsize=(10, 6))
# plt.plot(np.arange(1, num_episodes + 1), all_episode_steps_mean, label='Mean Steps', color='green')
# plt.fill_between(np.arange(1, num_episodes + 1),
#                  all_episode_steps_mean - all_episode_steps_std,
#                  all_episode_steps_mean + all_episode_steps_std,
#                  color='lightgreen', alpha=0.5, label='Standard Deviation')
# plt.title('Average Episode Steps vs. Number of Episodes in Cartpole', fontsize=14)
# plt.xlabel('Number of Episodes', fontsize=12)
# plt.ylabel('Average Steps', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()

# total_actions_taken = np.cumsum(all_episode_steps_mean)
# plt.plot(total_actions_taken, np.arange(1, num_episodes + 1), label='Learning Curve', color='orange')
# # plt.fill_between(np.arange(1, num_episodes + 1),
# #                  all_episode_steps_mean - all_episode_steps_std,
# #                  all_episode_steps_mean + all_episode_steps_std,
# #                  color='lightcoral', alpha=0.5, label='Standard Deviation')
# plt.title('Cumulative Actions Taken vs. Number of Episodes in Cartpole', fontsize=14)
# plt.xlabel('Cumulative Actions Taken', fontsize=12)
# plt.ylabel('Number of Episodes', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()

# env_name = 'CartPole-v1'
# # env.make(env_name)

# all_episode_rewards = []
# all_episode_steps = []

# gamma = 0.99
# alpha_theta = 0.01
# alpha_w = 0.01

# RUNS = 10
# num_episodes = 1000

# for run in range(RUNS):
#     print(f"Run: {run}")
#     env = gym.make(env_name)
#     # env.reset()

#     input_size = env.observation_space.shape[0]
#     hidden_size = 128
#     output_size = env.action_space.n

#     policy_net = PolicyNetwork(input_size, hidden_size, output_size)
#     state_value_net = StateValueNetwork(input_size, hidden_size)

#     policy_optimizer = torch.optim.Adam(policy_net.parameters(), lr=alpha_theta)
#     state_value_optimizer = torch.optim.Adam(state_value_net.parameters(), lr=alpha_w)

#     policy_net, state_value_net, episode_rewards, episode_steps = reinforce_with_baseline_cartpole(policy_net, state_value_net, policy_optimizer, state_value_optimizer, env, num_episodes, gamma)
#     all_episode_rewards.append(episode_rewards)
#     all_episode_steps.append(episode_steps)

# all_episode_rewards = np.array(all_episode_rewards)
# all_episode_steps = np.array(all_episode_steps)
# # print(all_episode_rewards)
# # print(all_episode_steps)

# all_episode_rewards_mean = all_episode_rewards.mean(axis=0)
# all_episode_steps_mean = all_episode_steps.mean(axis=0)

# all_episode_rewards_std = all_episode_rewards.std(axis=0)
# all_episode_steps_std = all_episode_steps.std(axis=0)

# plt.figure(figsize=(10, 6))
# plt.plot(np.arange(1, num_episodes + 1), all_episode_rewards_mean, label='Mean Reward', color='blue')
# plt.fill_between(np.arange(1, num_episodes + 1),
#                  all_episode_rewards_mean - all_episode_rewards_std,
#                  all_episode_rewards_mean + all_episode_rewards_std,
#                  color='lightblue', alpha=0.6, label='Standard Deviation')
# plt.title('Average Episode Reward vs. Number of Episodes in Cartpole', fontsize=14)
# plt.xlabel('Number of Episodes', fontsize=12)
# plt.ylabel('Average Reward per Episode', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()


# plt.figure(figsize=(10, 6))
# plt.plot(np.arange(1, num_episodes + 1), all_episode_steps_mean, label='Mean Steps', color='green')
# plt.fill_between(np.arange(1, num_episodes + 1),
#                  all_episode_steps_mean - all_episode_steps_std,
#                  all_episode_steps_mean + all_episode_steps_std,
#                  color='lightgreen', alpha=0.5, label='Standard Deviation')
# plt.title('Average Episode Steps vs. Number of Episodes in Cartpole', fontsize=14)
# plt.xlabel('Number of Episodes', fontsize=12)
# plt.ylabel('Average Steps', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()

# total_actions_taken = np.cumsum(all_episode_steps_mean)
# plt.plot(total_actions_taken, np.arange(1, num_episodes + 1), label='Learning Curve', color='orange')
# # plt.fill_between(np.arange(1, num_episodes + 1),
# #                  all_episode_steps_mean - all_episode_steps_std,
# #                  all_episode_steps_mean + all_episode_steps_std,
# #                  color='lightcoral', alpha=0.5, label='Standard Deviation')
# plt.title('Cumulative Actions Taken vs. Number of Episodes in Cartpole', fontsize=14)
# plt.xlabel('Cumulative Actions Taken', fontsize=12)
# plt.ylabel('Number of Episodes', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()

# env_name = 'CartPole-v1'
# # env.make(env_name)

# all_episode_rewards = []
# all_episode_steps = []

# gamma = 0.925
# alpha_theta = 0.01
# alpha_w = 0.01

# RUNS = 5
# num_episodes = 1000

# for run in range(RUNS):
#     print(f"Run: {run}")
#     env = gym.make(env_name)
#     # env.reset()

#     input_size = env.observation_space.shape[0]
#     hidden_size = 128
#     output_size = env.action_space.n

#     policy_net = PolicyNetwork(input_size, hidden_size, output_size)
#     state_value_net = StateValueNetwork(input_size, hidden_size)

#     policy_optimizer = torch.optim.Adam(policy_net.parameters(), lr=alpha_theta)
#     state_value_optimizer = torch.optim.Adam(state_value_net.parameters(), lr=alpha_w)

#     policy_net, state_value_net, episode_rewards, episode_steps = reinforce_with_baseline_cartpole(policy_net, state_value_net, policy_optimizer, state_value_optimizer, env, num_episodes, gamma)
#     all_episode_rewards.append(episode_rewards)
#     all_episode_steps.append(episode_steps)

# all_episode_rewards = np.array(all_episode_rewards)
# all_episode_steps = np.array(all_episode_steps)
# # print(all_episode_rewards)
# # print(all_episode_steps)

# all_episode_rewards_mean = all_episode_rewards.mean(axis=0)
# all_episode_steps_mean = all_episode_steps.mean(axis=0)

# all_episode_rewards_std = all_episode_rewards.std(axis=0)
# all_episode_steps_std = all_episode_steps.std(axis=0)

# plt.plot(np.arange(1, num_episodes + 1), all_episode_rewards_mean, label='Mean Reward (J)', color='blue')
# plt.fill_between(np.arange(1, num_episodes + 1),
#                  all_episode_rewards_mean - all_episode_rewards_std,
#                  all_episode_rewards_mean + all_episode_rewards_std,
#                  color='lightblue', alpha=0.5, label='Standard Deviation')

# plt.xlabel('Episode')
# plt.ylabel('Total Reward (J)')
# plt.title(f'Average Rewards over {RUNS} Runs for {env_name}')
# plt.legend()
# plt.show()


# plt.plot(np.arange(1, num_episodes + 1), all_episode_steps_mean, label='Mean Steps', color='green')
# plt.fill_between(np.arange(1, num_episodes + 1),
#                  all_episode_steps_mean - all_episode_steps_std,
#                  all_episode_steps_mean + all_episode_steps_std,
#                  color='lightgreen', alpha=0.5, label='Standard Deviation')

# plt.xlabel('Episode')
# plt.ylabel('Steps')
# plt.title(f'Average Steps over {RUNS} Runs for {env_name}')
# plt.legend()
# plt.show()

# total_actions_taken = np.cumsum(all_episode_steps_mean)
# plt.plot(total_actions_taken, np.arange(1, num_episodes + 1), label='Total Actions Taken Vs Episodes', color='orange')
# # plt.fill_between(np.arange(1, num_episodes + 1),
# #                  all_episode_steps_mean - all_episode_steps_std,
# #                  all_episode_steps_mean + all_episode_steps_std,
# #                  color='lightcoral', alpha=0.5, label='Standard Deviation')
# plt.xlabel('Total Actions Taken')
# plt.ylabel('Episode')
# plt.title(f'Learning Curve: Total Actions Taken over {RUNS} Runs for {env_name}')
# plt.legend()
# plt.show()

# env_name = 'CartPole-v1'
# # env.make(env_name)

# all_episode_rewards = []
# all_episode_steps = []

# gamma = 0.925
# alpha_theta = 0.01
# alpha_w = 0.01

# RUNS = 5
# num_episodes = 1000

# for run in range(RUNS):
#     print(f"Run: {run}")
#     env = gym.make(env_name)
#     # env.reset()

#     input_size = env.observation_space.shape[0]
#     hidden_size = 128
#     output_size = env.action_space.n

#     policy_net = PolicyNetwork(input_size, hidden_size, output_size)
#     state_value_net = StateValueNetwork(input_size, hidden_size)

#     policy_optimizer = torch.optim.Adam(policy_net.parameters(), lr=alpha_theta)
#     state_value_optimizer = torch.optim.Adam(state_value_net.parameters(), lr=alpha_w)

#     policy_net, state_value_net, episode_rewards, episode_steps = reinforce_with_baseline_cartpole(policy_net, state_value_net, policy_optimizer, state_value_optimizer, env, num_episodes, gamma)
#     all_episode_rewards.append(episode_rewards)
#     all_episode_steps.append(episode_steps)

# all_episode_rewards = np.array(all_episode_rewards)
# all_episode_steps = np.array(all_episode_steps)
# # print(all_episode_rewards)
# # print(all_episode_steps)

# all_episode_rewards_mean = all_episode_rewards.mean(axis=0)
# all_episode_steps_mean = all_episode_steps.mean(axis=0)

# all_episode_rewards_std = all_episode_rewards.std(axis=0)
# all_episode_steps_std = all_episode_steps.std(axis=0)

# plt.figure(figsize=(10, 6))
# plt.plot(np.arange(1, num_episodes + 1), all_episode_rewards_mean, label='Mean Reward', color='blue')
# plt.fill_between(np.arange(1, num_episodes + 1),
#                  all_episode_rewards_mean - all_episode_rewards_std,
#                  all_episode_rewards_mean + all_episode_rewards_std,
#                  color='lightblue', alpha=0.6, label='Standard Deviation')
# plt.title('Average Episode Reward vs. Number of Episodes in Cartpole', fontsize=14)
# plt.xlabel('Number of Episodes', fontsize=12)
# plt.ylabel('Average Reward per Episode', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()


# plt.figure(figsize=(10, 6))
# plt.plot(np.arange(1, num_episodes + 1), all_episode_steps_mean, label='Mean Steps', color='green')
# plt.fill_between(np.arange(1, num_episodes + 1),
#                  all_episode_steps_mean - all_episode_steps_std,
#                  all_episode_steps_mean + all_episode_steps_std,
#                  color='lightgreen', alpha=0.5, label='Standard Deviation')
# plt.title('Average Episode Steps vs. Number of Episodes in Cartpole', fontsize=14)
# plt.xlabel('Number of Episodes', fontsize=12)
# plt.ylabel('Average Steps', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()

# total_actions_taken = np.cumsum(all_episode_steps_mean)
# plt.plot(total_actions_taken, np.arange(1, num_episodes + 1), label='Learning Curve', color='orange')
# # plt.fill_between(np.arange(1, num_episodes + 1),
# #                  all_episode_steps_mean - all_episode_steps_std,
# #                  all_episode_steps_mean + all_episode_steps_std,
# #                  color='lightcoral', alpha=0.5, label='Standard Deviation')
# plt.title('Cumulative Actions Taken vs. Number of Episodes in Cartpole', fontsize=14)
# plt.xlabel('Cumulative Actions Taken', fontsize=12)
# plt.ylabel('Number of Episodes', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()

# env_name = 'CartPole-v1'
# # env.make(env_name)

# all_episode_rewards = []
# all_episode_steps = []

# gamma = 0.80
# alpha_theta = 0.01
# alpha_w = 0.01

# RUNS = 5
# num_episodes = 1000

# for run in range(RUNS):
#     print(f"Run: {run}")
#     env = gym.make(env_name)
#     # env.reset()

#     input_size = env.observation_space.shape[0]
#     hidden_size = 128
#     output_size = env.action_space.n

#     policy_net = PolicyNetwork(input_size, hidden_size, output_size)
#     state_value_net = StateValueNetwork(input_size, hidden_size)

#     policy_optimizer = torch.optim.Adam(policy_net.parameters(), lr=alpha_theta)
#     state_value_optimizer = torch.optim.Adam(state_value_net.parameters(), lr=alpha_w)

#     policy_net, state_value_net, episode_rewards, episode_steps = reinforce_with_baseline_cartpole(policy_net, state_value_net, policy_optimizer, state_value_optimizer, env, num_episodes, gamma)
#     all_episode_rewards.append(episode_rewards)
#     all_episode_steps.append(episode_steps)

# all_episode_rewards = np.array(all_episode_rewards)
# all_episode_steps = np.array(all_episode_steps)
# # print(all_episode_rewards)
# # print(all_episode_steps)

# all_episode_rewards_mean = all_episode_rewards.mean(axis=0)
# all_episode_steps_mean = all_episode_steps.mean(axis=0)

# all_episode_rewards_std = all_episode_rewards.std(axis=0)
# all_episode_steps_std = all_episode_steps.std(axis=0)

# plt.figure(figsize=(10, 6))
# plt.plot(np.arange(1, num_episodes + 1), all_episode_rewards_mean, label='Mean Reward', color='blue')
# plt.fill_between(np.arange(1, num_episodes + 1),
#                  all_episode_rewards_mean - all_episode_rewards_std,
#                  all_episode_rewards_mean + all_episode_rewards_std,
#                  color='lightblue', alpha=0.6, label='Standard Deviation')
# plt.title('Average Episode Reward vs. Number of Episodes in Cartpole', fontsize=14)
# plt.xlabel('Number of Episodes', fontsize=12)
# plt.ylabel('Average Reward per Episode', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()


# plt.figure(figsize=(10, 6))
# plt.plot(np.arange(1, num_episodes + 1), all_episode_steps_mean, label='Mean Steps', color='green')
# plt.fill_between(np.arange(1, num_episodes + 1),
#                  all_episode_steps_mean - all_episode_steps_std,
#                  all_episode_steps_mean + all_episode_steps_std,
#                  color='lightgreen', alpha=0.5, label='Standard Deviation')
# plt.title('Average Episode Steps vs. Number of Episodes in Cartpole', fontsize=14)
# plt.xlabel('Number of Episodes', fontsize=12)
# plt.ylabel('Average Steps', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()

# total_actions_taken = np.cumsum(all_episode_steps_mean)
# plt.plot(total_actions_taken, np.arange(1, num_episodes + 1), label='Learning Curve', color='orange')
# # plt.fill_between(np.arange(1, num_episodes + 1),
# #                  all_episode_steps_mean - all_episode_steps_std,
# #                  all_episode_steps_mean + all_episode_steps_std,
# #                  color='lightcoral', alpha=0.5, label='Standard Deviation')
# plt.title('Cumulative Actions Taken vs. Number of Episodes in Cartpole', fontsize=14)
# plt.xlabel('Cumulative Actions Taken', fontsize=12)
# plt.ylabel('Number of Episodes', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()

# env_name = 'CartPole-v1'
# # env.make(env_name)

# all_episode_rewards = []
# all_episode_steps = []

# gamma = 0.99
# alpha_theta = 0.001
# alpha_w = 0.001

# RUNS = 5
# num_episodes = 1000

# for run in range(RUNS):
#     print(f"Run: {run}")
#     env = gym.make(env_name)
#     # env.reset()

#     input_size = env.observation_space.shape[0]
#     hidden_size = 128
#     output_size = env.action_space.n

#     policy_net = PolicyNetwork(input_size, hidden_size, output_size)
#     state_value_net = StateValueNetwork(input_size, hidden_size)

#     policy_optimizer = torch.optim.Adam(policy_net.parameters(), lr=alpha_theta)
#     state_value_optimizer = torch.optim.Adam(state_value_net.parameters(), lr=alpha_w)

#     policy_net, state_value_net, episode_rewards, episode_steps = reinforce_with_baseline_cartpole(policy_net, state_value_net, policy_optimizer, state_value_optimizer, env, num_episodes, gamma)
#     all_episode_rewards.append(episode_rewards)
#     all_episode_steps.append(episode_steps)

# all_episode_rewards = np.array(all_episode_rewards)
# all_episode_steps = np.array(all_episode_steps)
# # print(all_episode_rewards)
# # print(all_episode_steps)

# all_episode_rewards_mean = all_episode_rewards.mean(axis=0)
# all_episode_steps_mean = all_episode_steps.mean(axis=0)

# all_episode_rewards_std = all_episode_rewards.std(axis=0)
# all_episode_steps_std = all_episode_steps.std(axis=0)

# plt.figure(figsize=(10, 6))
# plt.plot(np.arange(1, num_episodes + 1), all_episode_rewards_mean, label='Mean Reward', color='blue')
# plt.fill_between(np.arange(1, num_episodes + 1),
#                  all_episode_rewards_mean - all_episode_rewards_std,
#                  all_episode_rewards_mean + all_episode_rewards_std,
#                  color='lightblue', alpha=0.6, label='Standard Deviation')
# plt.title('Average Episode Reward vs. Number of Episodes in Cartpole', fontsize=14)
# plt.xlabel('Number of Episodes', fontsize=12)
# plt.ylabel('Average Reward per Episode', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()


# plt.figure(figsize=(10, 6))
# plt.plot(np.arange(1, num_episodes + 1), all_episode_steps_mean, label='Mean Steps', color='green')
# plt.fill_between(np.arange(1, num_episodes + 1),
#                  all_episode_steps_mean - all_episode_steps_std,
#                  all_episode_steps_mean + all_episode_steps_std,
#                  color='lightgreen', alpha=0.5, label='Standard Deviation')
# plt.title('Average Episode Steps vs. Number of Episodes in Cartpole', fontsize=14)
# plt.xlabel('Number of Episodes', fontsize=12)
# plt.ylabel('Average Steps', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()

# total_actions_taken = np.cumsum(all_episode_steps_mean)
# plt.plot(total_actions_taken, np.arange(1, num_episodes + 1), label='Learning Curve', color='orange')
# # plt.fill_between(np.arange(1, num_episodes + 1),
# #                  all_episode_steps_mean - all_episode_steps_std,
# #                  all_episode_steps_mean + all_episode_steps_std,
# #                  color='lightcoral', alpha=0.5, label='Standard Deviation')
# plt.title('Cumulative Actions Taken vs. Number of Episodes in Cartpole', fontsize=14)
# plt.xlabel('Cumulative Actions Taken', fontsize=12)
# plt.ylabel('Number of Episodes', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()

# env_name = 'CartPole-v1'
# # env.make(env_name)

# all_episode_rewards = []
# all_episode_steps = []

# gamma = 0.925
# alpha_theta = 0.001
# alpha_w = 0.001

# RUNS = 5
# num_episodes = 1000

# for run in range(RUNS):
#     print(f"Run: {run}")
#     env = gym.make(env_name)
#     # env.reset()

#     input_size = env.observation_space.shape[0]
#     hidden_size = 128
#     output_size = env.action_space.n

#     policy_net = PolicyNetwork(input_size, hidden_size, output_size)
#     state_value_net = StateValueNetwork(input_size, hidden_size)

#     policy_optimizer = torch.optim.Adam(policy_net.parameters(), lr=alpha_theta)
#     state_value_optimizer = torch.optim.Adam(state_value_net.parameters(), lr=alpha_w)

#     policy_net, state_value_net, episode_rewards, episode_steps = reinforce_with_baseline_cartpole(policy_net, state_value_net, policy_optimizer, state_value_optimizer, env, num_episodes, gamma)
#     all_episode_rewards.append(episode_rewards)
#     all_episode_steps.append(episode_steps)

# all_episode_rewards = np.array(all_episode_rewards)
# all_episode_steps = np.array(all_episode_steps)
# # print(all_episode_rewards)
# # print(all_episode_steps)

# all_episode_rewards_mean = all_episode_rewards.mean(axis=0)
# all_episode_steps_mean = all_episode_steps.mean(axis=0)

# all_episode_rewards_std = all_episode_rewards.std(axis=0)
# all_episode_steps_std = all_episode_steps.std(axis=0)

# plt.figure(figsize=(10, 6))
# plt.plot(np.arange(1, num_episodes + 1), all_episode_rewards_mean, label='Mean Reward', color='blue')
# plt.fill_between(np.arange(1, num_episodes + 1),
#                  all_episode_rewards_mean - all_episode_rewards_std,
#                  all_episode_rewards_mean + all_episode_rewards_std,
#                  color='lightblue', alpha=0.6, label='Standard Deviation')
# plt.title('Average Episode Reward vs. Number of Episodes in Cartpole', fontsize=14)
# plt.xlabel('Number of Episodes', fontsize=12)
# plt.ylabel('Average Reward per Episode', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()


# plt.figure(figsize=(10, 6))
# plt.plot(np.arange(1, num_episodes + 1), all_episode_steps_mean, label='Mean Steps', color='green')
# plt.fill_between(np.arange(1, num_episodes + 1),
#                  all_episode_steps_mean - all_episode_steps_std,
#                  all_episode_steps_mean + all_episode_steps_std,
#                  color='lightgreen', alpha=0.5, label='Standard Deviation')
# plt.title('Average Episode Steps vs. Number of Episodes in Cartpole', fontsize=14)
# plt.xlabel('Number of Episodes', fontsize=12)
# plt.ylabel('Average Steps', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()

# total_actions_taken = np.cumsum(all_episode_steps_mean)
# plt.plot(total_actions_taken, np.arange(1, num_episodes + 1), label='Learning Curve', color='orange')
# # plt.fill_between(np.arange(1, num_episodes + 1),
# #                  all_episode_steps_mean - all_episode_steps_std,
# #                  all_episode_steps_mean + all_episode_steps_std,
# #                  color='lightcoral', alpha=0.5, label='Standard Deviation')
# plt.title('Cumulative Actions Taken vs. Number of Episodes in Cartpole', fontsize=14)
# plt.xlabel('Cumulative Actions Taken', fontsize=12)
# plt.ylabel('Number of Episodes', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()

# env_name = 'CartPole-v1'
# # env.make(env_name)

# all_episode_rewards = []
# all_episode_steps = []

# gamma = 0.925
# alpha_theta = 0.3
# alpha_w = 0.3

# RUNS = 5
# num_episodes = 1000

# for run in range(RUNS):
#     print(f"Run: {run}")
#     env = gym.make(env_name)
#     # env.reset()

#     input_size = env.observation_space.shape[0]
#     hidden_size = 128
#     output_size = env.action_space.n

#     policy_net = PolicyNetwork(input_size, hidden_size, output_size)
#     state_value_net = StateValueNetwork(input_size, hidden_size)

#     policy_optimizer = torch.optim.Adam(policy_net.parameters(), lr=alpha_theta)
#     state_value_optimizer = torch.optim.Adam(state_value_net.parameters(), lr=alpha_w)

#     policy_net, state_value_net, episode_rewards, episode_steps = reinforce_with_baseline_cartpole(policy_net, state_value_net, policy_optimizer, state_value_optimizer, env, num_episodes, gamma)
#     all_episode_rewards.append(episode_rewards)
#     all_episode_steps.append(episode_steps)

# all_episode_rewards = np.array(all_episode_rewards)
# all_episode_steps = np.array(all_episode_steps)
# # print(all_episode_rewards)
# # print(all_episode_steps)

# all_episode_rewards_mean = all_episode_rewards.mean(axis=0)
# all_episode_steps_mean = all_episode_steps.mean(axis=0)

# all_episode_rewards_std = all_episode_rewards.std(axis=0)
# all_episode_steps_std = all_episode_steps.std(axis=0)

# plt.figure(figsize=(10, 6))
# plt.plot(np.arange(1, num_episodes + 1), all_episode_rewards_mean, label='Mean Reward', color='blue')
# plt.fill_between(np.arange(1, num_episodes + 1),
#                  all_episode_rewards_mean - all_episode_rewards_std,
#                  all_episode_rewards_mean + all_episode_rewards_std,
#                  color='lightblue', alpha=0.6, label='Standard Deviation')
# plt.title('Average Episode Reward vs. Number of Episodes in Cartpole', fontsize=14)
# plt.xlabel('Number of Episodes', fontsize=12)
# plt.ylabel('Average Reward per Episode', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()


# plt.figure(figsize=(10, 6))
# plt.plot(np.arange(1, num_episodes + 1), all_episode_steps_mean, label='Mean Steps', color='green')
# plt.fill_between(np.arange(1, num_episodes + 1),
#                  all_episode_steps_mean - all_episode_steps_std,
#                  all_episode_steps_mean + all_episode_steps_std,
#                  color='lightgreen', alpha=0.5, label='Standard Deviation')
# plt.title('Average Episode Steps vs. Number of Episodes in Cartpole', fontsize=14)
# plt.xlabel('Number of Episodes', fontsize=12)
# plt.ylabel('Average Steps', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()

# total_actions_taken = np.cumsum(all_episode_steps_mean)
# plt.plot(total_actions_taken, np.arange(1, num_episodes + 1), label='Learning Curve', color='orange')
# # plt.fill_between(np.arange(1, num_episodes + 1),
# #                  all_episode_steps_mean - all_episode_steps_std,
# #                  all_episode_steps_mean + all_episode_steps_std,
# #                  color='lightcoral', alpha=0.5, label='Standard Deviation')
# plt.title('Cumulative Actions Taken vs. Number of Episodes in Cartpole', fontsize=14)
# plt.xlabel('Cumulative Actions Taken', fontsize=12)
# plt.ylabel('Number of Episodes', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()

# env_name = 'CartPole-v1'
# # env.make(env_name)

# all_episode_rewards = []
# all_episode_steps = []

# gamma = 0.925
# alpha_theta = 0.01
# alpha_w = 0.01

# RUNS = 5
# num_episodes = 1000

# for run in range(RUNS):
#     print(f"Run: {run}")
#     env = gym.make(env_name)
#     # env.reset()

#     input_size = env.observation_space.shape[0]
#     hidden_size = 128
#     output_size = env.action_space.n

#     policy_net = PolicyNetwork(input_size, hidden_size, output_size)
#     state_value_net = StateValueNetwork(input_size, hidden_size)

#     policy_optimizer = torch.optim.Adam(policy_net.parameters(), lr=alpha_theta)
#     state_value_optimizer = torch.optim.Adam(state_value_net.parameters(), lr=alpha_w)

#     policy_net, state_value_net, episode_rewards, episode_steps = reinforce_with_baseline_cartpole(policy_net, state_value_net, policy_optimizer, state_value_optimizer, env, num_episodes, gamma)
#     all_episode_rewards.append(episode_rewards)
#     all_episode_steps.append(episode_steps)

# all_episode_rewards = np.array(all_episode_rewards)
# all_episode_steps = np.array(all_episode_steps)
# # print(all_episode_rewards)
# # print(all_episode_steps)

# all_episode_rewards_mean = all_episode_rewards.mean(axis=0)
# all_episode_steps_mean = all_episode_steps.mean(axis=0)

# all_episode_rewards_std = all_episode_rewards.std(axis=0)
# all_episode_steps_std = all_episode_steps.std(axis=0)

# plt.figure(figsize=(10, 6))
# plt.plot(np.arange(1, num_episodes + 1), all_episode_rewards_mean, label='Mean Reward', color='blue')
# plt.fill_between(np.arange(1, num_episodes + 1),
#                  all_episode_rewards_mean - all_episode_rewards_std,
#                  all_episode_rewards_mean + all_episode_rewards_std,
#                  color='lightblue', alpha=0.6, label='Standard Deviation')
# plt.title('Average Episode Reward vs. Number of Episodes in Cartpole', fontsize=14)
# plt.xlabel('Number of Episodes', fontsize=12)
# plt.ylabel('Average Reward per Episode', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()


# plt.figure(figsize=(10, 6))
# plt.plot(np.arange(1, num_episodes + 1), all_episode_steps_mean, label='Mean Steps', color='green')
# plt.fill_between(np.arange(1, num_episodes + 1),
#                  all_episode_steps_mean - all_episode_steps_std,
#                  all_episode_steps_mean + all_episode_steps_std,
#                  color='lightgreen', alpha=0.5, label='Standard Deviation')
# plt.title('Average Episode Steps vs. Number of Episodes in Cartpole', fontsize=14)
# plt.xlabel('Number of Episodes', fontsize=12)
# plt.ylabel('Average Steps', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()

# total_actions_taken = np.cumsum(all_episode_steps_mean)
# plt.plot(total_actions_taken, np.arange(1, num_episodes + 1), label='Learning Curve', color='orange')
# # plt.fill_between(np.arange(1, num_episodes + 1),
# #                  all_episode_steps_mean - all_episode_steps_std,
# #                  all_episode_steps_mean + all_episode_steps_std,
# #                  color='lightcoral', alpha=0.5, label='Standard Deviation')
# plt.title('Cumulative Actions Taken vs. Number of Episodes in Cartpole', fontsize=14)
# plt.xlabel('Cumulative Actions Taken', fontsize=12)
# plt.ylabel('Number of Episodes', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()

# env_name = 'CartPole-v1'
# # env.make(env_name)

# all_episode_rewards = []
# all_episode_steps = []

# gamma = 0.99
# alpha_theta = 0.01
# alpha_w = 0.01

# RUNS = 5
# num_episodes = 1000

# for run in range(RUNS):
#     print(f"Run: {run}")
#     env = gym.make(env_name)
#     # env.reset()

#     input_size = env.observation_space.shape[0]
#     hidden_size = 128
#     output_size = env.action_space.n

#     policy_net = PolicyNetwork(input_size, hidden_size, output_size)
#     state_value_net = StateValueNetwork(input_size, hidden_size)

#     policy_optimizer = torch.optim.Adam(policy_net.parameters(), lr=alpha_theta)
#     state_value_optimizer = torch.optim.Adam(state_value_net.parameters(), lr=alpha_w)

#     policy_net, state_value_net, episode_rewards, episode_steps = reinforce_with_baseline_cartpole(policy_net, state_value_net, policy_optimizer, state_value_optimizer, env, num_episodes, gamma)
#     all_episode_rewards.append(episode_rewards)
#     all_episode_steps.append(episode_steps)

# all_episode_rewards = np.array(all_episode_rewards)
# all_episode_steps = np.array(all_episode_steps)
# # print(all_episode_rewards)
# # print(all_episode_steps)

# all_episode_rewards_mean = all_episode_rewards.mean(axis=0)
# all_episode_steps_mean = all_episode_steps.mean(axis=0)

# all_episode_rewards_std = all_episode_rewards.std(axis=0)
# all_episode_steps_std = all_episode_steps.std(axis=0)

# plt.figure(figsize=(10, 6))
# plt.plot(np.arange(1, num_episodes + 1), all_episode_rewards_mean, label='Mean Reward', color='blue')
# plt.fill_between(np.arange(1, num_episodes + 1),
#                  all_episode_rewards_mean - all_episode_rewards_std,
#                  all_episode_rewards_mean + all_episode_rewards_std,
#                  color='lightblue', alpha=0.6, label='Standard Deviation')
# plt.title('Average Episode Reward vs. Number of Episodes in Cartpole', fontsize=14)
# plt.xlabel('Number of Episodes', fontsize=12)
# plt.ylabel('Average Reward per Episode', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()


# plt.figure(figsize=(10, 6))
# plt.plot(np.arange(1, num_episodes + 1), all_episode_steps_mean, label='Mean Steps', color='green')
# plt.fill_between(np.arange(1, num_episodes + 1),
#                  all_episode_steps_mean - all_episode_steps_std,
#                  all_episode_steps_mean + all_episode_steps_std,
#                  color='lightgreen', alpha=0.5, label='Standard Deviation')
# plt.title('Average Episode Steps vs. Number of Episodes in Cartpole', fontsize=14)
# plt.xlabel('Number of Episodes', fontsize=12)
# plt.ylabel('Average Steps', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()

# total_actions_taken = np.cumsum(all_episode_steps_mean)
# plt.plot(total_actions_taken, np.arange(1, num_episodes + 1), label='Learning Curve', color='orange')
# # plt.fill_between(np.arange(1, num_episodes + 1),
# #                  all_episode_steps_mean - all_episode_steps_std,
# #                  all_episode_steps_mean + all_episode_steps_std,
# #                  color='lightcoral', alpha=0.5, label='Standard Deviation')
# plt.title('Cumulative Actions Taken vs. Number of Episodes in Cartpole', fontsize=14)
# plt.xlabel('Cumulative Actions Taken', fontsize=12)
# plt.ylabel('Number of Episodes', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()

# env_name = 'CartPole-v1'
# # env.make(env_name)

# all_episode_rewards = []
# all_episode_steps = []

# gamma = 0.925
# alpha_theta = 0.001
# alpha_w = 0.001

# RUNS = 5
# num_episodes = 1000

# for run in range(RUNS):
#     print(f"Run: {run}")
#     env = gym.make(env_name)
#     # env.reset()

#     input_size = env.observation_space.shape[0]
#     hidden_size = 128
#     output_size = env.action_space.n

#     policy_net = PolicyNetwork(input_size, hidden_size, output_size)
#     state_value_net = StateValueNetwork(input_size, hidden_size)

#     policy_optimizer = torch.optim.Adam(policy_net.parameters(), lr=alpha_theta)
#     state_value_optimizer = torch.optim.Adam(state_value_net.parameters(), lr=alpha_w)

#     policy_net, state_value_net, episode_rewards, episode_steps = reinforce_with_baseline_cartpole(policy_net, state_value_net, policy_optimizer, state_value_optimizer, env, num_episodes, gamma)
#     all_episode_rewards.append(episode_rewards)
#     all_episode_steps.append(episode_steps)

# all_episode_rewards = np.array(all_episode_rewards)
# all_episode_steps = np.array(all_episode_steps)
# # print(all_episode_rewards)
# # print(all_episode_steps)

# all_episode_rewards_mean = all_episode_rewards.mean(axis=0)
# all_episode_steps_mean = all_episode_steps.mean(axis=0)

# all_episode_rewards_std = all_episode_rewards.std(axis=0)
# all_episode_steps_std = all_episode_steps.std(axis=0)

# plt.figure(figsize=(10, 6))
# plt.plot(np.arange(1, num_episodes + 1), all_episode_rewards_mean, label='Mean Reward', color='blue')
# plt.fill_between(np.arange(1, num_episodes + 1),
#                  all_episode_rewards_mean - all_episode_rewards_std,
#                  all_episode_rewards_mean + all_episode_rewards_std,
#                  color='lightblue', alpha=0.6, label='Standard Deviation')
# plt.title('Average Episode Reward vs. Number of Episodes in Cartpole', fontsize=14)
# plt.xlabel('Number of Episodes', fontsize=12)
# plt.ylabel('Average Reward per Episode', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()


# plt.figure(figsize=(10, 6))
# plt.plot(np.arange(1, num_episodes + 1), all_episode_steps_mean, label='Mean Steps', color='green')
# plt.fill_between(np.arange(1, num_episodes + 1),
#                  all_episode_steps_mean - all_episode_steps_std,
#                  all_episode_steps_mean + all_episode_steps_std,
#                  color='lightgreen', alpha=0.5, label='Standard Deviation')
# plt.title('Average Episode Steps vs. Number of Episodes in Cartpole', fontsize=14)
# plt.xlabel('Number of Episodes', fontsize=12)
# plt.ylabel('Average Steps', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()

# total_actions_taken = np.cumsum(all_episode_steps_mean)
# plt.plot(total_actions_taken, np.arange(1, num_episodes + 1), label='Learning Curve', color='orange')
# # plt.fill_between(np.arange(1, num_episodes + 1),
# #                  all_episode_steps_mean - all_episode_steps_std,
# #                  all_episode_steps_mean + all_episode_steps_std,
# #                  color='lightcoral', alpha=0.5, label='Standard Deviation')
# plt.title('Cumulative Actions Taken vs. Number of Episodes in Cartpole', fontsize=14)
# plt.xlabel('Cumulative Actions Taken', fontsize=12)
# plt.ylabel('Number of Episodes', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()

# """# **Acrobot-v1**"""

# def reinforce_with_baseline_acrobot(policy_net, state_value_net, policy_optimizer, state_value_optimizer, env, num_episodes, gamma):

#     all_episode_rewards = []
#     all_episode_steps = []

#     for episode in range(num_episodes):
#         state = env.reset()
#         state = torch.tensor(state, dtype=torch.float32)
#         done = False
#         timeout = False
#         episode_states = []
#         episode_actions = []
#         episode_rewards = []
#         episode_action_probs = []
#         episode_values = []
#         episode_steps = 0

#         while not (done or timeout or episode_steps>=2000):
#             action_probs = policy_net(state)
#             action = torch.multinomial(action_probs, 1).item()
#             # action_probs = action_probs.detach().numpy()[0]
#             # next_state, reward, done, timeout = env.step(action)
#             step_result = env.step(action)

#             reward = step_result[1]
#             done = step_result[2]
#             timeout = step_result[3]

#             next_state = step_result[0]
#             next_state = torch.tensor(next_state, dtype=torch.float32)

#             episode_states.append(state)
#             episode_action_probs.append(action_probs[action])
#             episode_actions.append(action)
#             episode_rewards.append(reward)

#             state = next_state
#             episode_steps += 1

#         # episode_states = torch.stack(episode_states)

#         returns_calculation = []
#         G = 0
#         episode_rewards_array = episode_rewards[::-1]
#         for r in (range(len(episode_rewards_array))):
#             reward = episode_rewards_array[r]
#             G = reward + gamma * G
#             returns_calculation.append(G)
#         # print("Episode Rewards (reversed)", episode_rewards)
#         returns_calculation = returns_calculation[::-1]
#         # print("Calculated Returns", returns_calculation)
#         returns_calculation = torch.tensor(returns_calculation, dtype=torch.float32)
#         # print("Episode Action Probs", episode_action_probs)

#         stacked_episode_states = torch.stack(episode_states)
#         stacked_episode_values = state_value_net(stacked_episode_states)
#         stacked_episode_val = stacked_episode_values.squeeze(1)
#         # print("Stacked Episode Values", stacked_episode_values)
#         # stacked_ep_val = stacked_episode_values.detach().numpy()
#         stacked_ep_val = stacked_episode_values.detach().numpy()
#         stacked_ep_val_flatten = stacked_ep_val.flatten()
#         # stacked_ep_val_tensor = torch.tensor(stacked_ep_val_flatten, dtype=torch.float32)
#         stacked_ep_val_tensor = torch.tensor(stacked_ep_val_flatten, dtype=torch.float32)
#         calculated_adv = returns_calculation - stacked_ep_val_tensor

#         policy_loss = []
#         for i in range(len(episode_action_probs)):
#             action_prob = episode_action_probs[i]
#             calculated_adv_value = calculated_adv[i]

#             prob_detached = action_prob.detach()
#             adv_detached = calculated_adv_value.detach()
#             log_prob = torch.log(prob_detached)

#             action_loss = -log_prob * adv_detached
#             policy_loss.append(action_loss)
#             # policy_loss.append(-torch.log(action_prob.detach()) * calculated_adv.detach())
#         policy_loss = torch.stack(policy_loss).sum()

#         policy_optimizer.zero_grad()
#         state_values_calculation = []
#         for state in episode_states:
#             state_value_net.eval()
#             state_values_calculation.append(state_value_net(state))
#         state_values_calculation = torch.cat([state_value_net(state) for state in episode_states])
#         advantages = returns_calculation - state_values_calculation.detach()
#         policy_optimizer_loss = compute_policy_gradient_loss(episode_action_probs, advantages)
#         # policy_optimizer_loss = -torch.stack(episode_action_probs).sum() * returns_calculation.mean()
#         # log_action_probs = torch.log(torch.stack(episode_action_probs))
#         # policy_optimizer_loss = -(log_action_probs * advantages).sum()
#         # policy_optimizer_loss = -torch.sum(torch.log(torch.stack(episode_action_probs)) * advantages)
#         # policy_optimizer_loss = -torch.stack(episode_action_probs).sum() * returns_calculation.mean()
#         policy_optimizer_loss.backward()
#         policy_optimizer.step()

#         state_value_optimizer.zero_grad()
#         # state_value_loss = F.mse_loss(state_value_net(state), returns_calculation)
#         episode_states_stack = torch.stack(episode_states)
#         state_values_calculation = state_value_net(episode_states_stack)
#         mse_loss_function = nn.MSELoss()
#         state_value_loss = mse_loss_function(state_values_calculation, returns_calculation)
#         state_value_loss.backward()
#         state_value_optimizer.step()

#         total_reward = sum(episode_rewards)
#         if (episode + 1) % 50 == 0:
#             print(f"Episode: {episode}, Reward: {total_reward}")
#             # print(f"Policy Loss: {policy_optimizer_loss.item()}, State Value Loss: {state_value_loss.item()}")

#         all_episode_rewards.append(total_reward)
#         all_episode_steps.append(episode_steps)

#     return policy_net, state_value_net, all_episode_rewards, all_episode_steps

# env_name = 'Acrobot-v1'
# # env = gym.make(env_name)

# all_episode_rewards = []
# all_episode_steps = []

# gamma = 0.80
# alpha_theta = 0.01
# alpha_w = 0.01

# RUNS = 5
# num_episodes = 1000

# for run in range(RUNS):
#     print(f"Run: {run}")
#     env = gym.make(env_name)
#     # env.reset()

#     input_size = env.observation_space.shape[0]
#     hidden_size = 256
#     output_size = env.action_space.n

#     policy_net = PolicyNetwork(input_size, hidden_size, output_size)
#     state_value_net = StateValueNetwork(input_size, hidden_size)

#     policy_optimizer = torch.optim.Adam(policy_net.parameters(), lr=alpha_theta)
#     state_value_optimizer = torch.optim.Adam(state_value_net.parameters(), lr=alpha_w)

#     policy_net, state_value_net, episode_rewards, episode_steps = reinforce_with_baseline_acrobot(policy_net, state_value_net, policy_optimizer, state_value_optimizer, env, num_episodes, gamma)
#     all_episode_rewards.append(episode_rewards)
#     all_episode_steps.append(episode_steps)

# all_episode_rewards = np.array(all_episode_rewards)
# all_episode_steps = np.array(all_episode_steps)
# # print(all_episode_rewards)
# # print(all_episode_steps)

# all_episode_rewards_mean = all_episode_rewards.mean(axis=0)
# all_episode_steps_mean = all_episode_steps.mean(axis=0)

# all_episode_rewards_std = all_episode_rewards.std(axis=0)
# all_episode_steps_std = all_episode_steps.std(axis=0)

# plt.figure(figsize=(10, 6))
# plt.plot(np.arange(1, num_episodes + 1), all_episode_rewards_mean, label='Mean Reward', color='blue')
# plt.fill_between(np.arange(1, num_episodes + 1),
#                  all_episode_rewards_mean - all_episode_rewards_std,
#                  all_episode_rewards_mean + all_episode_rewards_std,
#                  color='lightblue', alpha=0.6, label='Standard Deviation')
# plt.title('Average Episode Reward vs. Number of Episodes in Acrobot', fontsize=14)
# plt.xlabel('Number of Episodes', fontsize=12)
# plt.ylabel('Average Reward per Episode', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()


# plt.figure(figsize=(10, 6))
# plt.plot(np.arange(1, num_episodes + 1), all_episode_steps_mean, label='Mean Steps', color='green')
# plt.fill_between(np.arange(1, num_episodes + 1),
#                  all_episode_steps_mean - all_episode_steps_std,
#                  all_episode_steps_mean + all_episode_steps_std,
#                  color='lightgreen', alpha=0.5, label='Standard Deviation')
# plt.title('Average Episode Steps vs. Number of Episodes in Acrobot', fontsize=14)
# plt.xlabel('Number of Episodes', fontsize=12)
# plt.ylabel('Average Steps', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()

# total_actions_taken = np.cumsum(all_episode_steps_mean)
# plt.plot(total_actions_taken, np.arange(1, num_episodes + 1), label='Learning Curve', color='orange')
# # plt.fill_between(np.arange(1, num_episodes + 1),
# #                  all_episode_steps_mean - all_episode_steps_std,
# #                  all_episode_steps_mean + all_episode_steps_std,
# #                  color='lightcoral', alpha=0.5, label='Standard Deviation')
# plt.title('Cumulative Actions Taken vs. Number of Episodes in Acrobot', fontsize=14)
# plt.xlabel('Cumulative Actions Taken', fontsize=12)
# plt.ylabel('Number of Episodes', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()

# env_name = 'Acrobot-v1'
# # env = gym.make(env_name)

# all_episode_rewards = []
# all_episode_steps = []

# gamma = 0.99
# alpha_theta = 0.01
# alpha_w = 0.01

# RUNS = 5
# num_episodes = 1000

# for run in range(RUNS):
#     print(f"Run: {run}")
#     env = gym.make(env_name)
#     # env.reset()

#     input_size = env.observation_space.shape[0]
#     hidden_size = 256
#     output_size = env.action_space.n

#     policy_net = PolicyNetwork(input_size, hidden_size, output_size)
#     state_value_net = StateValueNetwork(input_size, hidden_size)

#     policy_optimizer = torch.optim.Adam(policy_net.parameters(), lr=alpha_theta)
#     state_value_optimizer = torch.optim.Adam(state_value_net.parameters(), lr=alpha_w)

#     policy_net, state_value_net, episode_rewards, episode_steps = reinforce_with_baseline_acrobot(policy_net, state_value_net, policy_optimizer, state_value_optimizer, env, num_episodes, gamma)
#     all_episode_rewards.append(episode_rewards)
#     all_episode_steps.append(episode_steps)

# all_episode_rewards = np.array(all_episode_rewards)
# all_episode_steps = np.array(all_episode_steps)
# # print(all_episode_rewards)
# # print(all_episode_steps)

# all_episode_rewards_mean = all_episode_rewards.mean(axis=0)
# all_episode_steps_mean = all_episode_steps.mean(axis=0)

# all_episode_rewards_std = all_episode_rewards.std(axis=0)
# all_episode_steps_std = all_episode_steps.std(axis=0)

# plt.figure(figsize=(10, 6))
# plt.plot(np.arange(1, num_episodes + 1), all_episode_rewards_mean, label='Mean Reward', color='blue')
# plt.fill_between(np.arange(1, num_episodes + 1),
#                  all_episode_rewards_mean - all_episode_rewards_std,
#                  all_episode_rewards_mean + all_episode_rewards_std,
#                  color='lightblue', alpha=0.6, label='Standard Deviation')
# plt.title('Average Episode Reward vs. Number of Episodes in Acrobot', fontsize=14)
# plt.xlabel('Number of Episodes', fontsize=12)
# plt.ylabel('Average Reward per Episode', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()


# plt.figure(figsize=(10, 6))
# plt.plot(np.arange(1, num_episodes + 1), all_episode_steps_mean, label='Mean Steps', color='green')
# plt.fill_between(np.arange(1, num_episodes + 1),
#                  all_episode_steps_mean - all_episode_steps_std,
#                  all_episode_steps_mean + all_episode_steps_std,
#                  color='lightgreen', alpha=0.5, label='Standard Deviation')
# plt.title('Average Episode Steps vs. Number of Episodes in Acrobot', fontsize=14)
# plt.xlabel('Number of Episodes', fontsize=12)
# plt.ylabel('Average Steps', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()

# total_actions_taken = np.cumsum(all_episode_steps_mean)
# plt.plot(total_actions_taken, np.arange(1, num_episodes + 1), label='Learning Curve', color='orange')
# # plt.fill_between(np.arange(1, num_episodes + 1),
# #                  all_episode_steps_mean - all_episode_steps_std,
# #                  all_episode_steps_mean + all_episode_steps_std,
# #                  color='lightcoral', alpha=0.5, label='Standard Deviation')
# plt.title('Cumulative Actions Taken vs. Number of Episodes in Acrobot', fontsize=14)
# plt.xlabel('Cumulative Actions Taken', fontsize=12)
# plt.ylabel('Number of Episodes', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()

# env_name = 'Acrobot-v1'
# # env = gym.make(env_name)

# all_episode_rewards = []
# all_episode_steps = []

# gamma = 0.925
# alpha_theta = 0.0003
# alpha_w = 0.0003

# RUNS = 5
# num_episodes = 1000

# for run in range(RUNS):
#     print(f"Run: {run}")
#     env = gym.make(env_name)
#     # env.reset()

#     input_size = env.observation_space.shape[0]
#     hidden_size = 256
#     output_size = env.action_space.n

#     policy_net = PolicyNetwork(input_size, hidden_size, output_size)
#     state_value_net = StateValueNetwork(input_size, hidden_size)

#     policy_optimizer = torch.optim.Adam(policy_net.parameters(), lr=alpha_theta)
#     state_value_optimizer = torch.optim.Adam(state_value_net.parameters(), lr=alpha_w)

#     policy_net, state_value_net, episode_rewards, episode_steps = reinforce_with_baseline_acrobot(policy_net, state_value_net, policy_optimizer, state_value_optimizer, env, num_episodes, gamma)
#     all_episode_rewards.append(episode_rewards)
#     all_episode_steps.append(episode_steps)

# all_episode_rewards = np.array(all_episode_rewards)
# all_episode_steps = np.array(all_episode_steps)
# # print(all_episode_rewards)
# # print(all_episode_steps)

# all_episode_rewards_mean = all_episode_rewards.mean(axis=0)
# all_episode_steps_mean = all_episode_steps.mean(axis=0)

# all_episode_rewards_std = all_episode_rewards.std(axis=0)
# all_episode_steps_std = all_episode_steps.std(axis=0)

# plt.figure(figsize=(10, 6))
# plt.plot(np.arange(1, num_episodes + 1), all_episode_rewards_mean, label='Mean Reward', color='blue')
# plt.fill_between(np.arange(1, num_episodes + 1),
#                  all_episode_rewards_mean - all_episode_rewards_std,
#                  all_episode_rewards_mean + all_episode_rewards_std,
#                  color='lightblue', alpha=0.6, label='Standard Deviation')
# plt.title('Average Episode Reward vs. Number of Episodes in Acrobot', fontsize=14)
# plt.xlabel('Number of Episodes', fontsize=12)
# plt.ylabel('Average Reward per Episode', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()


# plt.figure(figsize=(10, 6))
# plt.plot(np.arange(1, num_episodes + 1), all_episode_steps_mean, label='Mean Steps', color='green')
# plt.fill_between(np.arange(1, num_episodes + 1),
#                  all_episode_steps_mean - all_episode_steps_std,
#                  all_episode_steps_mean + all_episode_steps_std,
#                  color='lightgreen', alpha=0.5, label='Standard Deviation')
# plt.title('Average Episode Steps vs. Number of Episodes in Acrobot', fontsize=14)
# plt.xlabel('Number of Episodes', fontsize=12)
# plt.ylabel('Average Steps', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()

# total_actions_taken = np.cumsum(all_episode_steps_mean)
# plt.plot(total_actions_taken, np.arange(1, num_episodes + 1), label='Learning Curve', color='orange')
# # plt.fill_between(np.arange(1, num_episodes + 1),
# #                  all_episode_steps_mean - all_episode_steps_std,
# #                  all_episode_steps_mean + all_episode_steps_std,
# #                  color='lightcoral', alpha=0.5, label='Standard Deviation')
# plt.title('Cumulative Actions Taken vs. Number of Episodes in Acrobot', fontsize=14)
# plt.xlabel('Cumulative Actions Taken', fontsize=12)
# plt.ylabel('Number of Episodes', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()

# env_name = 'Acrobot-v1'
# # env = gym.make(env_name)

# all_episode_rewards = []
# all_episode_steps = []

# gamma = 0.925
# alpha_theta = 0.003
# alpha_w = 0.003

# RUNS = 5
# num_episodes = 1000

# for run in range(RUNS):
#     print(f"Run: {run}")
#     env = gym.make(env_name)
#     # env.reset()

#     input_size = env.observation_space.shape[0]
#     hidden_size = 256
#     output_size = env.action_space.n

#     policy_net = PolicyNetwork(input_size, hidden_size, output_size)
#     state_value_net = StateValueNetwork(input_size, hidden_size)

#     policy_optimizer = torch.optim.Adam(policy_net.parameters(), lr=alpha_theta)
#     state_value_optimizer = torch.optim.Adam(state_value_net.parameters(), lr=alpha_w)

#     policy_net, state_value_net, episode_rewards, episode_steps = reinforce_with_baseline_acrobot(policy_net, state_value_net, policy_optimizer, state_value_optimizer, env, num_episodes, gamma)
#     all_episode_rewards.append(episode_rewards)
#     all_episode_steps.append(episode_steps)

# all_episode_rewards = np.array(all_episode_rewards)
# all_episode_steps = np.array(all_episode_steps)
# # print(all_episode_rewards)
# # print(all_episode_steps)

# all_episode_rewards_mean = all_episode_rewards.mean(axis=0)
# all_episode_steps_mean = all_episode_steps.mean(axis=0)

# all_episode_rewards_std = all_episode_rewards.std(axis=0)
# all_episode_steps_std = all_episode_steps.std(axis=0)

# plt.figure(figsize=(10, 6))
# plt.plot(np.arange(1, num_episodes + 1), all_episode_rewards_mean, label='Mean Reward', color='blue')
# plt.fill_between(np.arange(1, num_episodes + 1),
#                  all_episode_rewards_mean - all_episode_rewards_std,
#                  all_episode_rewards_mean + all_episode_rewards_std,
#                  color='lightblue', alpha=0.6, label='Standard Deviation')
# plt.title('Average Episode Reward vs. Number of Episodes in Acrobot', fontsize=14)
# plt.xlabel('Number of Episodes', fontsize=12)
# plt.ylabel('Average Reward per Episode', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()


# plt.figure(figsize=(10, 6))
# plt.plot(np.arange(1, num_episodes + 1), all_episode_steps_mean, label='Mean Steps', color='green')
# plt.fill_between(np.arange(1, num_episodes + 1),
#                  all_episode_steps_mean - all_episode_steps_std,
#                  all_episode_steps_mean + all_episode_steps_std,
#                  color='lightgreen', alpha=0.5, label='Standard Deviation')
# plt.title('Average Episode Steps vs. Number of Episodes in Acrobot', fontsize=14)
# plt.xlabel('Number of Episodes', fontsize=12)
# plt.ylabel('Average Steps', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()

# total_actions_taken = np.cumsum(all_episode_steps_mean)
# plt.plot(total_actions_taken, np.arange(1, num_episodes + 1), label='Learning Curve', color='orange')
# # plt.fill_between(np.arange(1, num_episodes + 1),
# #                  all_episode_steps_mean - all_episode_steps_std,
# #                  all_episode_steps_mean + all_episode_steps_std,
# #                  color='lightcoral', alpha=0.5, label='Standard Deviation')
# plt.title('Cumulative Actions Taken vs. Number of Episodes in Acrobot', fontsize=14)
# plt.xlabel('Cumulative Actions Taken', fontsize=12)
# plt.ylabel('Number of Episodes', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()

# """# **LunarLander**"""

# pip install --upgrade gym

# """WARNING: The following packages were previously imported in this runtime:
#   [gym]
# You must restart the runtime in order to use newly installed versions.
# """

# pip install swig

# pip install gymnasium[box2d]

# def reinforce_with_baseline_lunarlander(policy_net, state_value_net, policy_optimizer, state_value_optimizer, env, num_episodes, gamma):

#     all_episode_rewards = []
#     all_episode_steps = []

#     for episode in range(num_episodes):
#         state = env.reset()
#         state = torch.tensor(state, dtype=torch.float32)
#         done = False
#         timeout = False
#         episode_states = []
#         episode_actions = []
#         episode_rewards = []
#         episode_action_probs = []
#         episode_values = []
#         episode_steps = 0

#         while not (done or timeout):
#             action_probs = policy_net(state)
#             action = torch.multinomial(action_probs, 1).item()
#             # action_probs = action_probs.detach().numpy()[0]
#             # next_state, reward, done, timeout = env.step(action)
#             step_result = env.step(action)

#             reward = step_result[1]
#             done = step_result[2]
#             timeout = step_result[3]

#             next_state = step_result[0]
#             next_state = torch.tensor(next_state, dtype=torch.float32)

#             episode_states.append(state)
#             episode_action_probs.append(action_probs[action])
#             episode_actions.append(action)
#             episode_rewards.append(reward)

#             state = next_state
#             episode_steps += 1

#         # episode_states = torch.stack(episode_states)

#         returns_calculation = []
#         G = 0
#         episode_rewards_array = episode_rewards[::-1]
#         for r in (range(len(episode_rewards_array))):
#             reward = episode_rewards_array[r]
#             G = reward + gamma * G
#             returns_calculation.append(G)
#         # print("Episode Rewards (reversed)", episode_rewards)
#         returns_calculation = returns_calculation[::-1]
#         # print("Calculated Returns", returns_calculation)
#         returns_calculation = torch.tensor(returns_calculation, dtype=torch.float32)
#         # print("Episode Action Probs", episode_action_probs)

#         stacked_episode_states = torch.stack(episode_states)
#         stacked_episode_values = state_value_net(stacked_episode_states)
#         stacked_episode_val = stacked_episode_values.squeeze(1)
#         # print("Stacked Episode Values", stacked_episode_values)
#         # stacked_ep_val = stacked_episode_values.detach().numpy()
#         stacked_ep_val = stacked_episode_values.detach().numpy()
#         stacked_ep_val_flatten = stacked_ep_val.flatten()
#         # stacked_ep_val_tensor = torch.tensor(stacked_ep_val_flatten, dtype=torch.float32)
#         stacked_ep_val_tensor = torch.tensor(stacked_ep_val_flatten, dtype=torch.float32)
#         calculated_adv = returns_calculation - stacked_ep_val_tensor

#         policy_loss = []
#         for i in range(len(episode_action_probs)):
#             action_prob = episode_action_probs[i]
#             calculated_adv_value = calculated_adv[i]

#             prob_detached = action_prob.detach()
#             adv_detached = calculated_adv_value.detach()
#             log_prob = torch.log(prob_detached)

#             action_loss = -log_prob * adv_detached
#             policy_loss.append(action_loss)
#             # policy_loss.append(-torch.log(action_prob.detach()) * calculated_adv.detach())
#         policy_loss = torch.stack(policy_loss).sum()

#         policy_optimizer.zero_grad()
#         state_values_calculation = []
#         for state in episode_states:
#             state_value_net.eval()
#             state_values_calculation.append(state_value_net(state))
#         state_values_calculation = torch.cat([state_value_net(state) for state in episode_states])
#         advantages = returns_calculation - state_values_calculation.detach()
#         policy_optimizer_loss = compute_policy_gradient_loss(episode_action_probs, advantages)
#         # policy_optimizer_loss = -torch.stack(episode_action_probs).sum() * returns_calculation.mean()
#         # log_action_probs = torch.log(torch.stack(episode_action_probs))
#         # policy_optimizer_loss = -(log_action_probs * advantages).sum()
#         # policy_optimizer_loss = -torch.sum(torch.log(torch.stack(episode_action_probs)) * advantages)
#         # policy_optimizer_loss = -torch.stack(episode_action_probs).sum() * returns_calculation.mean()
#         policy_optimizer_loss.backward()
#         policy_optimizer.step()

#         state_value_optimizer.zero_grad()
#         # state_value_loss = F.mse_loss(state_value_net(state), returns_calculation)
#         episode_states_stack = torch.stack(episode_states)
#         state_values_calculation = state_value_net(episode_states_stack)
#         mse_loss_function = nn.MSELoss()
#         state_value_loss = mse_loss_function(state_values_calculation, returns_calculation)
#         state_value_loss.backward()
#         state_value_optimizer.step()

#         total_reward = sum(episode_rewards)
#         if (episode + 1) % 50 == 0:
#             print(f"Episode: {episode}, Reward: {total_reward}")
#             # print(f"Policy Loss: {policy_optimizer_loss.item()}, State Value Loss: {state_value_loss.item()}")

#         all_episode_rewards.append(total_reward)
#         all_episode_steps.append(episode_steps)

#     return policy_net, state_value_net, all_episode_rewards, all_episode_steps

# env_name = 'LunarLander-v2'
# # env = gym.make(env_name)

# all_episode_rewards = []
# all_episode_steps = []

# gamma = 0.99
# alpha_theta = 0.0003
# alpha_w = 0.0003

# RUNS = 5
# num_episodes = 1000

# for run in range(RUNS):
#     print(f"Run: {run}")
#     env = gym.make(env_name)
#     # env.reset()

#     input_size = env.observation_space.shape[0]
#     hidden_size = 256
#     output_size = env.action_space.n

#     policy_net = PolicyNetwork(input_size, hidden_size, output_size)
#     state_value_net = StateValueNetwork(input_size, hidden_size)

#     policy_optimizer = torch.optim.Adam(policy_net.parameters(), lr=alpha_theta)
#     state_value_optimizer = torch.optim.Adam(state_value_net.parameters(), lr=alpha_w)

#     policy_net, state_value_net, episode_rewards, episode_steps = reinforce_with_baseline_lunarlander(policy_net, state_value_net, policy_optimizer, state_value_optimizer, env, num_episodes, gamma)
#     all_episode_rewards.append(episode_rewards)
#     all_episode_steps.append(episode_steps)

# all_episode_rewards = np.array(all_episode_rewards)
# all_episode_steps = np.array(all_episode_steps)
# # print(all_episode_rewards)
# # print(all_episode_steps)

# all_episode_rewards_mean = all_episode_rewards.mean(axis=0)
# all_episode_steps_mean = all_episode_steps.mean(axis=0)

# all_episode_rewards_std = all_episode_rewards.std(axis=0)
# all_episode_steps_std = all_episode_steps.std(axis=0)

# plt.figure(figsize=(10, 6))
# plt.plot(np.arange(1, num_episodes + 1), all_episode_rewards_mean, label='Average(Mean) Reward', color='blue')
# plt.fill_between(np.arange(1, num_episodes + 1),
#                  all_episode_rewards_mean - all_episode_rewards_std,
#                  all_episode_rewards_mean + all_episode_rewards_std,
#                  color='lightblue', alpha=0.6, label='Standard Deviation')
# plt.title('Average Episode Reward vs. Number of Episodes in Lunar Lander', fontsize=14)
# plt.xlabel('Number of Episodes', fontsize=12)
# plt.ylabel('Average Reward per Episode', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()


# plt.figure(figsize=(10, 6))
# plt.plot(np.arange(1, num_episodes + 1), all_episode_steps_mean, label='Mean Steps', color='green')
# plt.fill_between(np.arange(1, num_episodes + 1),
#                  all_episode_steps_mean - all_episode_steps_std,
#                  all_episode_steps_mean + all_episode_steps_std,
#                  color='lightgreen', alpha=0.5, label='Standard Deviation')
# plt.title('Average Episode Steps vs. Number of Episodes in Lunar Lander', fontsize=14)
# plt.xlabel('Number of Episodes', fontsize=12)
# plt.ylabel('Average Steps', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()

# total_actions_taken = np.cumsum(all_episode_steps_mean)
# plt.plot(total_actions_taken, np.arange(1, num_episodes + 1), label='Learning Curve', color='orange')
# # plt.fill_between(np.arange(1, num_episodes + 1),
# #                  all_episode_steps_mean - all_episode_steps_std,
# #                  all_episode_steps_mean + all_episode_steps_std,
# #                  color='lightcoral', alpha=0.5, label='Standard Deviation')
# plt.title('Cumulative Actions Taken vs. Number of Episodes in Lunar Lander', fontsize=14)
# plt.xlabel('Cumulative Actions Taken', fontsize=12)
# plt.ylabel('Number of Episodes', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()

# env_name = 'LunarLander-v3'
# # env = gym.make(env_name)

# all_episode_rewards = []
# all_episode_steps = []

# gamma = 0.925
# alpha_theta = 0.003
# alpha_w = 0.003

# RUNS = 5
# num_episodes = 1000

# for run in range(RUNS):
#     print(f"Run: {run}")
#     env = gym.make(env_name)
#     # env.reset()

#     input_size = env.observation_space.shape[0]
#     hidden_size = 256
#     output_size = env.action_space.n

#     policy_net = PolicyNetwork(input_size, hidden_size, output_size)
#     state_value_net = StateValueNetwork(input_size, hidden_size)

#     policy_optimizer = torch.optim.Adam(policy_net.parameters(), lr=alpha_theta)
#     state_value_optimizer = torch.optim.Adam(state_value_net.parameters(), lr=alpha_w)

#     policy_net, state_value_net, episode_rewards, episode_steps = reinforce_with_baseline_lunarlander(policy_net, state_value_net, policy_optimizer, state_value_optimizer, env, num_episodes, gamma)
#     all_episode_rewards.append(episode_rewards)
#     all_episode_steps.append(episode_steps)

# all_episode_rewards = np.array(all_episode_rewards)
# all_episode_steps = np.array(all_episode_steps)
# # print(all_episode_rewards)
# # print(all_episode_steps)

# all_episode_rewards_mean = all_episode_rewards.mean(axis=0)
# all_episode_steps_mean = all_episode_steps.mean(axis=0)

# all_episode_rewards_std = all_episode_rewards.std(axis=0)
# all_episode_steps_std = all_episode_steps.std(axis=0)

# plt.figure(figsize=(10, 6))
# plt.plot(np.arange(1, num_episodes + 1), all_episode_rewards_mean, label='Mean Reward', color='blue')
# plt.fill_between(np.arange(1, num_episodes + 1),
#                  all_episode_rewards_mean - all_episode_rewards_std,
#                  all_episode_rewards_mean + all_episode_rewards_std,
#                  color='lightblue', alpha=0.6, label='Standard Deviation')
# plt.title('Average Episode Reward vs. Number of Episodes in Lunar Lander', fontsize=14)
# plt.xlabel('Number of Episodes', fontsize=12)
# plt.ylabel('Average Reward per Episode', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()


# plt.figure(figsize=(10, 6))
# plt.plot(np.arange(1, num_episodes + 1), all_episode_steps_mean, label='Mean Steps', color='green')
# plt.fill_between(np.arange(1, num_episodes + 1),
#                  all_episode_steps_mean - all_episode_steps_std,
#                  all_episode_steps_mean + all_episode_steps_std,
#                  color='lightgreen', alpha=0.5, label='Standard Deviation')
# plt.title('Average Episode Steps vs. Number of Episodes in Lunar Lander', fontsize=14)
# plt.xlabel('Number of Episodes', fontsize=12)
# plt.ylabel('Average Steps', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()

# total_actions_taken = np.cumsum(all_episode_steps_mean)
# plt.plot(total_actions_taken, np.arange(1, num_episodes + 1), label='Learning Curve', color='orange')
# # plt.fill_between(np.arange(1, num_episodes + 1),
# #                  all_episode_steps_mean - all_episode_steps_std,
# #                  all_episode_steps_mean + all_episode_steps_std,
# #                  color='lightcoral', alpha=0.5, label='Standard Deviation')
# plt.title('Cumulative Actions Taken vs. Number of Episodes in Lunar Lander', fontsize=14)
# plt.xlabel('Cumulative Actions Taken', fontsize=12)
# plt.ylabel('Number of Episodes', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()

# env_name = 'LunarLander-v3'
# # env = gym.make(env_name)

# all_episode_rewards = []
# all_episode_steps = []

# gamma = 0.925
# alpha_theta = 0.0003
# alpha_w = 0.0003

# RUNS = 5
# num_episodes = 1000

# for run in range(RUNS):
#     print(f"Run: {run}")
#     env = gym.make("LunarLander-v3")
#     # env.reset()

#     input_size = env.observation_space.shape[0]
#     hidden_size = 256
#     output_size = env.action_space.n

#     policy_net = PolicyNetwork(input_size, hidden_size, output_size)
#     state_value_net = StateValueNetwork(input_size, hidden_size)

#     policy_optimizer = torch.optim.Adam(policy_net.parameters(), lr=alpha_theta)
#     state_value_optimizer = torch.optim.Adam(state_value_net.parameters(), lr=alpha_w)

#     policy_net, state_value_net, episode_rewards, episode_steps = reinforce_with_baseline_lunarlander(policy_net, state_value_net, policy_optimizer, state_value_optimizer, env, num_episodes, gamma)
#     all_episode_rewards.append(episode_rewards)
#     all_episode_steps.append(episode_steps)

# all_episode_rewards = np.array(all_episode_rewards)
# all_episode_steps = np.array(all_episode_steps)
# # print(all_episode_rewards)
# # print(all_episode_steps)

# all_episode_rewards_mean = all_episode_rewards.mean(axis=0)
# all_episode_steps_mean = all_episode_steps.mean(axis=0)

# all_episode_rewards_std = all_episode_rewards.std(axis=0)
# all_episode_steps_std = all_episode_steps.std(axis=0)

# plt.figure(figsize=(10, 6))
# plt.plot(np.arange(1, num_episodes + 1), all_episode_rewards_mean, label='Mean Reward', color='blue')
# plt.fill_between(np.arange(1, num_episodes + 1),
#                  all_episode_rewards_mean - all_episode_rewards_std,
#                  all_episode_rewards_mean + all_episode_rewards_std,
#                  color='lightblue', alpha=0.6, label='Standard Deviation')
# plt.title('Average Episode Reward vs. Number of Episodes in Lunar Lander', fontsize=14)
# plt.xlabel('Number of Episodes', fontsize=12)
# plt.ylabel('Average Reward per Episode', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()


# plt.figure(figsize=(10, 6))
# plt.plot(np.arange(1, num_episodes + 1), all_episode_steps_mean, label='Mean Steps', color='green')
# plt.fill_between(np.arange(1, num_episodes + 1),
#                  all_episode_steps_mean - all_episode_steps_std,
#                  all_episode_steps_mean + all_episode_steps_std,
#                  color='lightgreen', alpha=0.5, label='Standard Deviation')
# plt.title('Average Episode Steps vs. Number of Episodes in Lunar Lander', fontsize=14)
# plt.xlabel('Number of Episodes', fontsize=12)
# plt.ylabel('Average Steps', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()

# total_actions_taken = np.cumsum(all_episode_steps_mean)
# plt.plot(total_actions_taken, np.arange(1, num_episodes + 1), label='Learning Curve', color='orange')
# # plt.fill_between(np.arange(1, num_episodes + 1),
# #                  all_episode_steps_mean - all_episode_steps_std,
# #                  all_episode_steps_mean + all_episode_steps_std,
# #                  color='lightcoral', alpha=0.5, label='Standard Deviation')
# plt.title('Cumulative Actions Taken vs. Number of Episodes in Lunar Lander', fontsize=14)
# plt.xlabel('Cumulative Actions Taken', fontsize=12)
# plt.ylabel('Number of Episodes', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()

# env_name = 'LunarLander-v2'
# # env = gym.make(env_name)

# all_episode_rewards = []
# all_episode_steps = []

# gamma = 0.925
# alpha_theta = 0.03
# alpha_w = 0.03

# RUNS = 5
# num_episodes = 1000

# for run in range(RUNS):
#     print(f"Run: {run}")
#     env = gym.make(env_name)
#     # env.reset()

#     input_size = env.observation_space.shape[0]
#     hidden_size = 256
#     output_size = env.action_space.n

#     policy_net = PolicyNetwork(input_size, hidden_size, output_size)
#     state_value_net = StateValueNetwork(input_size, hidden_size)

#     policy_optimizer = torch.optim.Adam(policy_net.parameters(), lr=alpha_theta)
#     state_value_optimizer = torch.optim.Adam(state_value_net.parameters(), lr=alpha_w)

#     policy_net, state_value_net, episode_rewards, episode_steps = reinforce_with_baseline_lunarlander(policy_net, state_value_net, policy_optimizer, state_value_optimizer, env, num_episodes, gamma)
#     all_episode_rewards.append(episode_rewards)
#     all_episode_steps.append(episode_steps)

# all_episode_rewards = np.array(all_episode_rewards)
# all_episode_steps = np.array(all_episode_steps)
# # print(all_episode_rewards)
# # print(all_episode_steps)

# all_episode_rewards_mean = all_episode_rewards.mean(axis=0)
# all_episode_steps_mean = all_episode_steps.mean(axis=0)

# all_episode_rewards_std = all_episode_rewards.std(axis=0)
# all_episode_steps_std = all_episode_steps.std(axis=0)

# plt.figure(figsize=(10, 6))
# plt.plot(np.arange(1, num_episodes + 1), all_episode_rewards_mean, label='Mean Reward', color='blue')
# plt.fill_between(np.arange(1, num_episodes + 1),
#                  all_episode_rewards_mean - all_episode_rewards_std,
#                  all_episode_rewards_mean + all_episode_rewards_std,
#                  color='lightblue', alpha=0.6, label='Standard Deviation')
# plt.title('Average Episode Reward vs. Number of Episodes in Lunar Lander', fontsize=14)
# plt.xlabel('Number of Episodes', fontsize=12)
# plt.ylabel('Average Reward per Episode', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()


# plt.figure(figsize=(10, 6))
# plt.plot(np.arange(1, num_episodes + 1), all_episode_steps_mean, label='Mean Steps', color='green')
# plt.fill_between(np.arange(1, num_episodes + 1),
#                  all_episode_steps_mean - all_episode_steps_std,
#                  all_episode_steps_mean + all_episode_steps_std,
#                  color='lightgreen', alpha=0.5, label='Standard Deviation')
# plt.title('Average Episode Steps vs. Number of Episodes in Lunar Lander', fontsize=14)
# plt.xlabel('Number of Episodes', fontsize=12)
# plt.ylabel('Average Steps', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()

# total_actions_taken = np.cumsum(all_episode_steps_mean)
# plt.plot(total_actions_taken, np.arange(1, num_episodes + 1), label='Learning Curve', color='orange')
# # plt.fill_between(np.arange(1, num_episodes + 1),
# #                  all_episode_steps_mean - all_episode_steps_std,
# #                  all_episode_steps_mean + all_episode_steps_std,
# #                  color='lightcoral', alpha=0.5, label='Standard Deviation')
# plt.title('Cumulative Actions Taken vs. Number of Episodes in Lunar Lander', fontsize=14)
# plt.xlabel('Cumulative Actions Taken', fontsize=12)
# plt.ylabel('Number of Episodes', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()

# env_name = 'LunarLander-v2'
# # env = gym.make(env_name)

# all_episode_rewards = []
# all_episode_steps = []

# gamma = 0.925
# alpha_theta = 0.3
# alpha_w = 0.3

# RUNS = 5
# num_episodes = 1000

# for run in range(RUNS):
#     print(f"Run: {run}")
#     env = gym.make(env_name)
#     # env.reset()

#     input_size = env.observation_space.shape[0]
#     hidden_size = 256
#     output_size = env.action_space.n

#     policy_net = PolicyNetwork(input_size, hidden_size, output_size)
#     state_value_net = StateValueNetwork(input_size, hidden_size)

#     policy_optimizer = torch.optim.Adam(policy_net.parameters(), lr=alpha_theta)
#     state_value_optimizer = torch.optim.Adam(state_value_net.parameters(), lr=alpha_w)

#     policy_net, state_value_net, episode_rewards, episode_steps = reinforce_with_baseline_lunarlander(policy_net, state_value_net, policy_optimizer, state_value_optimizer, env, num_episodes, gamma)
#     all_episode_rewards.append(episode_rewards)
#     all_episode_steps.append(episode_steps)

# all_episode_rewards = np.array(all_episode_rewards)
# all_episode_steps = np.array(all_episode_steps)
# # print(all_episode_rewards)
# # print(all_episode_steps)

# all_episode_rewards_mean = all_episode_rewards.mean(axis=0)
# all_episode_steps_mean = all_episode_steps.mean(axis=0)

# all_episode_rewards_std = all_episode_rewards.std(axis=0)
# all_episode_steps_std = all_episode_steps.std(axis=0)

# plt.figure(figsize=(10, 6))
# plt.plot(np.arange(1, num_episodes + 1), all_episode_rewards_mean, label='Mean Reward', color='blue')
# plt.fill_between(np.arange(1, num_episodes + 1),
#                  all_episode_rewards_mean - all_episode_rewards_std,
#                  all_episode_rewards_mean + all_episode_rewards_std,
#                  color='lightblue', alpha=0.6, label='Standard Deviation')
# plt.title('Average Episode Reward vs. Number of Episodes in Lunar Lander', fontsize=14)
# plt.xlabel('Number of Episodes', fontsize=12)
# plt.ylabel('Average Reward per Episode', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()


# plt.figure(figsize=(10, 6))
# plt.plot(np.arange(1, num_episodes + 1), all_episode_steps_mean, label='Mean Steps', color='green')
# plt.fill_between(np.arange(1, num_episodes + 1),
#                  all_episode_steps_mean - all_episode_steps_std,
#                  all_episode_steps_mean + all_episode_steps_std,
#                  color='lightgreen', alpha=0.5, label='Standard Deviation')
# plt.title('Average Episode Steps vs. Number of Episodes in Lunar Lander', fontsize=14)
# plt.xlabel('Number of Episodes', fontsize=12)
# plt.ylabel('Average Steps', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()

# total_actions_taken = np.cumsum(all_episode_steps_mean)
# plt.plot(total_actions_taken, np.arange(1, num_episodes + 1), label='Learning Curve', color='orange')
# # plt.fill_between(np.arange(1, num_episodes + 1),
# #                  all_episode_steps_mean - all_episode_steps_std,
# #                  all_episode_steps_mean + all_episode_steps_std,
# #                  color='lightcoral', alpha=0.5, label='Standard Deviation')
# plt.title('Cumulative Actions Taken vs. Number of Episodes in Lunar Lander', fontsize=14)
# plt.xlabel('Cumulative Actions Taken', fontsize=12)
# plt.ylabel('Number of Episodes', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()