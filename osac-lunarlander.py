# -*- coding: utf-8 -*-
"""RL-Project_ActorCritic.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_SOBtaiMaiEqaBhOLYfVforSPmMOIn1A

# **One-Step Actor-Critic Algorithm**

# **LunarLander**

# pip install --upgrade gym

WARNING: The following packages were previously imported in this runtime:
  [gym]
You must restart the runtime in order to use newly installed versions.

# pip install swig
# pip install gymnasium[box2d]

To Run:
# python3 osac-lunarlander.py

"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import gym
import numpy as np
import matplotlib.pyplot as plt
from torch.distributions import Categorical
from math import pi, cos

class FourierFeatures(nn.Module):
    def __init__(self, num_dims, num_orders):
        super(FourierFeatures, self).__init__()
        self.num_dims = num_dims
        self.num_orders = num_orders

        coefficients = []
        for i in range(num_dims):
            c = []
            for j in range(num_orders):
                c.append(nn.Parameter(torch.randn(1)))
            # self.add_module(f'c{i}', nn.ParameterList(c))
            coefficients.append(nn.ParameterList(c))
        self.coefficients = nn.ParameterList(coefficients)
        self.weights = nn.ParameterList([nn.Parameter(torch.randn(1)) for _ in range(num_dims)])
        self.biases = nn.ParameterList([nn.Parameter(torch.randn(1)) for _ in range(num_dims)])

    def normalize_state(self, state):
        min_max_val = [
            [-4.8, 4.8],
            [-4.0, 4.0],
            [-0.418, 0.418],
            [-2.5, 2.5]
        ]
        normalized_state = []
        for i in range(len(state)):
            min_val = min_max_val[i][0]
            max_val = min_max_val[i][1]
            normalized_val = (state[i] - min_val) / (max_val - min_val)
            normalized_state.append(normalized_val)

        normalized_state = torch.stack(normalized_state)

        return normalized_state

    def forward(self, x):
        # state_features = np.array(x)
        features = []
        state_features = x
        state_features_normalized = self.normalize_state(state_features)
        # state_features = torch.matmul(state_features, self.weights) + self.biases
        for i in range(self.num_dims):
            for j in range(self.num_orders):
                coefficient = self.coefficients[i][j]
                angle = coefficient * state_features_normalized[:, i] + self.biases[i]

                cos_feature = torch.cos(angle)
                features.append(self.weights[i] * cos_feature)

                # sin_feature = torch.sin(angle)
                # features.append(self.weights[i] * sin_feature)

        cos_state_features = torch.stack(features, dim=1)
        # cos_state_features = np.repeat(cos_state_features, self.features_count, axis=1)
        # sin_state_features = torch.stack(features, dim=1)
        # sin_state_features = np.repeat(sin_state_features, self.features_count, axis=1)
        return cos_state_features

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 6, 5)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.conv3 = nn.Conv2d(8, 32, 5)
        self.fc1 = nn.Linear(32 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))
        x = F.max_pool2d(F.relu(self.conv2(x)), 2)
        x = F.max_pool2d(F.relu(self.conv3(x)), 2)
        x = x.view(-1, self.num_flat_features(x))
        x = F.relu(self.fc1(x))

class PolicyNetwork(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(PolicyNetwork, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, output_size)
        # self.softmax = nn.Softmax(dim=-1)
        # self.relu = nn.ReLU()
        # self.dropout = nn.Dropout(p=0.5)
        # self.batchnorm = nn.BatchNorm1d(hidden_size)
        # self.init_weights()

    # def init_weights(self):
    #     for m in self.modules():
    #         if isinstance(m, nn.Linear):
    #             nn.init.xavier_uniform_(m.weight)
    #             nn.init.constant_(m.bias, 0)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = torch.softmax(self.fc3(x), dim=-1)
        # x = self.relu(x)
        # x = self.dropout(x)
        # x = self.batchnorm(x)
        # x = F.relu(x)
        return x

class StateValueNetwork(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(StateValueNetwork, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, 1)
        # self.softmax = nn.Softmax(dim=-1)
        # self.dropout = nn.Dropout(p=0.5)
        # self.batchnorm = nn.BatchNorm1d(hidden_size)
        # self.init_weights()

    # def init_weights(self):
    #     for m in self.modules():
    #         if isinstance(m, nn.Linear):
    #             nn.init.xavier_uniform_(m.weight)
    #             nn.init.constant_(m.bias, 0)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = torch.relu(self.fc3(x))
        # x = self.dropout(x)
        # x = self.batchnorm(x)
        # x = F.relu(x)
        # x = self.softmax(x)
        return x

class ActorCriticLunarLander(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, fourier_order):
        super(ActorCriticLunarLander, self).__init__()
        self.actor_policy_network = PolicyNetwork(input_size * fourier_order, hidden_size, output_size)
        self.actor_state_value_network = StateValueNetwork(input_size * fourier_order, hidden_size)
        self.fourier_features = FourierFeatures(input_size, fourier_order)

        self.statemean = torch.zeros(input_size)
        self.statestd = torch.ones(input_size)
        self.actionmean = torch.zeros(output_size)
        self.actionstd = torch.ones(output_size)
        self.action_space = output_size
        self.state_space = input_size
        self.hidden_size = hidden_size
        self.fourier_order = fourier_order

    def normalize_state(self, state):
        min_max_val = [
            [-1, 1],
            [0, 1],
            [-2.0, 2.0],
            [-pi, pi],
            [-2*pi, 2*pi],
            [0, 1],
            [0, 1]
        ]
        normalized_state = []
        for i in range(len(state)):
            normalized_state.append((state[i] - min_max_val[i][0]) / (min_max_val[i][1] - min_max_val[i][0]))

        normalized_state = torch.stack(normalized_state)
        return normalized_state

    # def normalize_state(self, state):
    #     return (state - self.statemean) / self.statestd

    def normalize_action(self, action):
        return (action - self.actionmean) / self.actionstd

    def forward(self, x):
        state_features = x
        state_features = self.normalize_state(state_features)
        features = self.fourier_features(state_features)
        actor_policy = self.actor_policy_network(features)
        actor_state_value = self.actor_state_value_network(features)
        return actor_policy, actor_state_value

actions = [0, 1, 2, 3]

def action_selection(state, actor_critic_model, sigma):
    state = torch.tensor(state, dtype=torch.float32)
    state = state.unsqueeze(0)
    actor_policy, actor_state_value = actor_critic_model(state)
    state_exploration = sigma * actor_policy
    action_probs = torch.softmax(state_exploration, dim=-1)
    # action_probs_detach = actor_policy.detach().numpy()[0]
    # action_probs_avg = np.exp(action_probs) / np.sum(np.exp(action_probs))
    action_probs_dist = Categorical(action_probs)
    dist = torch.distributions.Categorical(logits=action_probs)
    action = dist.sample()
    return action.item(), dist.log_prob(action), action_probs

# def action_selection_test(state, actor_critic_model, sigma):
#     state = torch.tensor(state, dtype=torch.float32)
#     state = state.unsqueeze(0)

def update_networks(policy_optimizer, actor_policy_loss, state_value_optimizer, critic_state_value_loss):

    policy_optimizer.zero_grad()
    actor_policy_loss.backward()
    policy_optimizer.step()

    state_value_optimizer.zero_grad()
    critic_state_value_loss.backward()
    state_value_optimizer.step()

def actor_critic_function(policy_net, state_value_net, policy_optimizer, state_value_optimizer, env, model, num_episodes, gamma, sigma):

    all_episode_rewards = []
    all_episode_steps = []

    for episode in range(num_episodes):
        state = env.reset()
        state = torch.tensor(state, dtype=torch.float32)
        done = False
        timeout = False
        episode_states = []
        episode_actions = []
        episode_rewards = []
        episode_values = []
        episode_reward = 0
        episode_steps = 0

        while not (done or timeout):
            action, dist_log_prob, action_probs = action_selection(state, model, sigma)

            step_result = env.step(action)
            next_state = step_result[0]
            reward = step_result[1]
            done = step_result[2]
            timeout = step_result[3]

            state_value = state_value_net(state)
            next_state = torch.tensor(next_state, dtype=torch.float32)
            next_state_value = state_value_net(next_state)

            current_cumulative_reward = reward + gamma * next_state_value - state_value
            # current_cumulative_reward = current_cumulative_reward - state_value
            action_prob = dist_log_prob.exp()

            # action_index = actions.index(action)
            # action_prob = action_prob[action_index]

            detach_current_cumulative_reward = current_cumulative_reward.detach()
            actor_policy_loss = -dist_log_prob * detach_current_cumulative_reward
            critic_state_value_loss = current_cumulative_reward.pow(2)

            update_networks(policy_optimizer, actor_policy_loss, state_value_optimizer, critic_state_value_loss)

            episode_states.append(state)
            episode_actions.append(action)
            episode_rewards.append(reward)
            episode_values.append(dist_log_prob)

            state = next_state
            state = torch.tensor(state, dtype=torch.float32)
            episode_reward += reward
            episode_steps += 1

        if episode % 100 == 0:
            print(f"Episode {episode}/{num_episodes}, Total Reward: {episode_reward}")

        all_episode_rewards.append(episode_reward)
        all_episode_steps.append(episode_steps)

    return policy_net, state_value_net, all_episode_rewards, all_episode_steps

env = gym.make('LunarLander-v2')
env.reset()

state_dimension = 4
action_dimension = 64

input_size = env.observation_space.shape[0]
hidden_size = 128
output_size = env.action_space.n
fourier_order = 3

model = ActorCriticLunarLander(input_size, hidden_size, output_size, fourier_order)
policy_net = PolicyNetwork(input_size, hidden_size, output_size)
state_value_net = StateValueNetwork(input_size, hidden_size)
fourier_features = FourierFeatures(state_dimension, action_dimension)

policy_net.eval()
state_value_net.eval()

#Parameters lr
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
policy_optimizer = torch.optim.Adam(policy_net.parameters(), lr=0.001)
state_value_optimizer = torch.optim.Adam(state_value_net.parameters(), lr=0.001)

print(model)
print(policy_net)
print(state_value_net)

"""# **LunarLander**"""

env_name = 'LunarLander-v2'
# env.make(env_name)

all_episode_rewards = []
all_episode_steps = []

gamma = 0.925
alpha_theta = 0.0001
alpha_w = 0.0001

fourier_order = 3

sigma = 1.0

RUNS = 5
num_episodes = 1000

for run in range(RUNS):
    print(f"Run: {run}")
    env = gym.make(env_name)
    # env.reset()

    input_size = env.observation_space.shape[0]
    hidden_size = 128
    output_size = env.action_space.n

    model = ActorCriticLunarLander(input_size, hidden_size, output_size, fourier_order)
    policy_net = PolicyNetwork(input_size, hidden_size, output_size)
    state_value_net = StateValueNetwork(input_size, hidden_size)

    policy_net.eval()
    state_value_net.eval()

    #Parameters lr
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    policy_optimizer = torch.optim.Adam(policy_net.parameters(), lr=0.001)
    state_value_optimizer = torch.optim.Adam(state_value_net.parameters(), lr=0.001)

    # print(model)
    # print(policy_net)
    # print(state_value_net)

    policy_net, state_value_net, episode_rewards, episode_steps = actor_critic_function(policy_net, state_value_net, policy_optimizer, state_value_optimizer, env, model, num_episodes, gamma, sigma)
    all_episode_rewards.append(episode_rewards)
    all_episode_steps.append(episode_steps)

all_episode_rewards_array = np.array(all_episode_rewards)
all_episode_steps_array = np.array(all_episode_steps)
# print(all_episode_rewards)
# print(all_episode_steps)

all_episode_rewards_mean = np.mean(all_episode_rewards_array, axis=0)
all_episode_steps_mean = np.mean(all_episode_steps_array, axis=0)

all_episode_rewards_std = np.std(all_episode_rewards_array, axis=0)
all_episode_steps_std = np.std(all_episode_steps_array, axis=0)

plt.figure(figsize=(12, 6))
plt.plot(np.arange(1, num_episodes + 1), all_episode_rewards_mean, label='Average(Mean) Reward', color='blue')
plt.fill_between(np.arange(1, num_episodes + 1),
                 all_episode_rewards_mean - all_episode_rewards_std,
                 all_episode_rewards_mean + all_episode_rewards_std,
                 color='orange', alpha=0.5, label='Standard Deviation')
plt.title('Average Episode Reward vs. Number of Episodes in Lunar Lander', fontsize=14)
plt.xlabel('Number of Episodes', fontsize=12)
plt.ylabel('Average Reward per Episode', fontsize=12)
plt.legend(fontsize=10)
plt.show()

plt.figure(figsize=(12, 6))
plt.plot(np.arange(1, num_episodes + 1), all_episode_steps_mean, label='Average Steps', color='green')
plt.fill_between(np.arange(1, num_episodes + 1),
                 all_episode_steps_mean - all_episode_steps_std,
                 all_episode_steps_mean + all_episode_steps_std,
                 color='lightgreen', alpha=0.5, label='Standard Deviation')
plt.title('Average Episode Steps vs. Number of Episodes in Lunar Lander', fontsize=14)
plt.xlabel('Number of Episodes', fontsize=12)
plt.ylabel('Average Steps per Episode', fontsize=12)
plt.legend(fontsize=10)
plt.show()


plt.figure(figsize=(12, 6))
total_actions_taken = np.cumsum(all_episode_steps_mean)
plt.plot(total_actions_taken, np.arange(1, num_episodes + 1), label='Learning Curve', color='green')
plt.title('Cumulative Actions Taken vs. Number of Episodes in Lunar Lander', fontsize=14)
plt.xlabel('Cumulative Actions Taken', fontsize=12)
plt.ylabel('Number of Episodes', fontsize=12)
plt.legend(fontsize=10)
plt.show()
# env.close()

# """# **Cartpole**"""

# env_name = 'CartPole-v1'
# # env.make(env_name)

# all_episode_rewards = []
# all_episode_steps = []

# gamma = 0.925
# alpha_theta = 0.003
# alpha_w = 0.003

# fourier_order = 3

# sigma = 1.0

# RUNS = 5
# num_episodes = 1000

# for run in range(RUNS):
#     print(f"Run: {run}")
#     env = gym.make(env_name)
#     # env.reset()

#     input_size = env.observation_space.shape[0]
#     hidden_size = 64
#     output_size = env.action_space.n

#     model = ActorCriticCartpole(input_size, hidden_size, output_size, fourier_order)
#     policy_net = PolicyNetwork(input_size, hidden_size, output_size)
#     state_value_net = StateValueNetwork(input_size, hidden_size)

#     policy_net.eval()
#     state_value_net.eval()

#     #Parameters lr
#     optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
#     policy_optimizer = torch.optim.Adam(policy_net.parameters(), lr=0.001)
#     state_value_optimizer = torch.optim.Adam(state_value_net.parameters(), lr=0.001)

#     # print(model)
#     # print(policy_net)
#     # print(state_value_net)

#     policy_net, state_value_net, episode_rewards, episode_steps = actor_critic_function(policy_net, state_value_net, policy_optimizer, state_value_optimizer, env, model, num_episodes, gamma, sigma)
#     all_episode_rewards.append(episode_rewards)
#     all_episode_steps.append(episode_steps)

# all_episode_rewards_array = np.array(all_episode_rewards)
# all_episode_steps_array = np.array(all_episode_steps)
# # print(all_episode_rewards)
# # print(all_episode_steps)

# all_episode_rewards_mean = np.mean(all_episode_rewards_array, axis=0)
# all_episode_steps_mean = np.mean(all_episode_steps_array, axis=0)

# all_episode_rewards_std = np.std(all_episode_rewards_array, axis=0)
# all_episode_steps_std = np.std(all_episode_steps_array, axis=0)

# plt.figure(figsize=(12, 6))
# plt.plot(np.arange(1, num_episodes + 1), all_episode_rewards_mean, label='Average(Mean) Reward', color='blue')
# plt.fill_between(np.arange(1, num_episodes + 1),
#                  all_episode_rewards_mean - all_episode_rewards_std,
#                  all_episode_rewards_mean + all_episode_rewards_std,
#                  color='orange', alpha=0.5, label='Standard Deviation')
# plt.title('Average Episode Reward vs. Number of Episodes in CartPole', fontsize=14)
# plt.xlabel('Number of Episodes', fontsize=12)
# plt.ylabel('Average Reward per Episode', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()

# plt.figure(figsize=(12, 6))
# plt.plot(np.arange(1, num_episodes + 1), all_episode_steps_mean, label='Mean Steps', color='green')
# plt.fill_between(np.arange(1, num_episodes + 1),
#                  all_episode_steps_mean - all_episode_steps_std,
#                  all_episode_steps_mean + all_episode_steps_std,
#                  color='lightgreen', alpha=0.5, label='Standard Deviation')
# plt.title('Average Episode Steps vs. Number of Episodes in CartPole', fontsize=14)
# plt.xlabel('Number of Episodes', fontsize=12)
# plt.ylabel('Average Steps per Episode', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()


# plt.figure(figsize=(12, 6))
# total_actions_taken = np.cumsum(all_episode_steps_mean)
# plt.plot(total_actions_taken, np.arange(1, num_episodes + 1), label='Learning Curve', color='green')
# plt.title('Cumulative Actions Taken vs. Number of Episodes in CartPole', fontsize=14)
# plt.xlabel('Cumulative Actions Taken', fontsize=12)
# plt.ylabel('Number of Episodes', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()
# # env.close()

# env_name = 'CartPole-v1'
# # env.make(env_name)

# all_episode_rewards = []
# all_episode_steps = []

# gamma = 0.925
# alpha_theta = 0.003
# alpha_w = 0.003

# fourier_order = 3

# sigma = 1.0

# RUNS = 5
# num_episodes = 1000

# for run in range(RUNS):
#     print(f"Run: {run}")
#     env = gym.make(env_name)
#     # env.reset()

#     input_size = env.observation_space.shape[0]
#     hidden_size = 64
#     output_size = env.action_space.n

#     model = ActorCriticCartpole(input_size, hidden_size, output_size)
#     policy_net = PolicyNetwork(input_size, hidden_size, output_size)
#     state_value_net = StateValueNetwork(input_size, hidden_size)

#     policy_net.eval()
#     state_value_net.eval()

#     #Parameters lr
#     optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
#     policy_optimizer = torch.optim.Adam(policy_net.parameters(), lr=0.001)
#     state_value_optimizer = torch.optim.Adam(state_value_net.parameters(), lr=0.001)

#     # print(model)
#     # print(policy_net)
#     # print(state_value_net)

#     policy_net, state_value_net, episode_rewards, episode_steps = actor_critic_function(policy_net, state_value_net, policy_optimizer, state_value_optimizer, env, model, num_episodes, gamma, sigma)
#     all_episode_rewards.append(episode_rewards)
#     all_episode_steps.append(episode_steps)

# all_episode_rewards_array = np.array(all_episode_rewards)
# all_episode_steps_array = np.array(all_episode_steps)
# # print(all_episode_rewards)
# # print(all_episode_steps)

# all_episode_rewards_mean = np.mean(all_episode_rewards_array, axis=0)
# all_episode_steps_mean = np.mean(all_episode_steps_array, axis=0)

# all_episode_rewards_std = np.std(all_episode_rewards_array, axis=0)
# all_episode_steps_std = np.std(all_episode_steps_array, axis=0)

# plt.figure(figsize=(12, 6))
# plt.plot(np.arange(1, num_episodes + 1), all_episode_rewards_mean, label='Average(Mean) Reward', color='blue')
# plt.fill_between(np.arange(1, num_episodes + 1),
#                  all_episode_rewards_mean - all_episode_rewards_std,
#                  all_episode_rewards_mean + all_episode_rewards_std,
#                  color='orange', alpha=0.5, label='Standard Deviation')
# plt.title('Average Episode Reward vs. Number of Episodes in CartPole', fontsize=14)
# plt.xlabel('Number of Episodes', fontsize=12)
# plt.ylabel('Average Reward per Episode', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()

# plt.figure(figsize=(12, 6))
# plt.plot(np.arange(1, num_episodes + 1), all_episode_steps_mean, label='Mean Steps', color='green')
# plt.fill_between(np.arange(1, num_episodes + 1),
#                  all_episode_steps_mean - all_episode_steps_std,
#                  all_episode_steps_mean + all_episode_steps_std,
#                  color='lightgreen', alpha=0.5, label='Standard Deviation')
# plt.title('Average Episode Steps vs. Number of Episodes in CartPole', fontsize=14)
# plt.xlabel('Number of Episodes', fontsize=12)
# plt.ylabel('Average Steps per Episode', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()


# plt.figure(figsize=(12, 6))
# total_actions_taken = np.cumsum(all_episode_steps_mean)
# plt.plot(total_actions_taken, np.arange(1, num_episodes + 1), label='Learning Curve', color='green')
# plt.title('Cumulative Actions Taken vs. Number of Episodes in CartPole', fontsize=14)
# plt.xlabel('Cumulative Actions Taken', fontsize=12)
# plt.ylabel('Number of Episodes', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()
# # env.close()

# env_name = 'CartPole-v1'
# # env.make(env_name)

# all_episode_rewards = []
# all_episode_steps = []

# gamma = 0.925
# alpha_theta = 0.03
# alpha_w = 0.03

# fourier_order = 3

# sigma = 1.0

# RUNS = 5
# num_episodes = 1000

# for run in range(RUNS):
#     print(f"Run: {run}")
#     env = gym.make(env_name)
#     # env.reset()

#     input_size = env.observation_space.shape[0]
#     hidden_size = 64
#     output_size = env.action_space.n

#     model = ActorCriticCartpole(input_size, hidden_size, output_size)
#     policy_net = PolicyNetwork(input_size, hidden_size, output_size)
#     state_value_net = StateValueNetwork(input_size, hidden_size)

#     policy_net.eval()
#     state_value_net.eval()

#     #Parameters lr
#     optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
#     policy_optimizer = torch.optim.Adam(policy_net.parameters(), lr=0.001)
#     state_value_optimizer = torch.optim.Adam(state_value_net.parameters(), lr=0.001)

#     # print(model)
#     # print(policy_net)
#     # print(state_value_net)

#     policy_net, state_value_net, episode_rewards, episode_steps = actor_critic_function(policy_net, state_value_net, policy_optimizer, state_value_optimizer, env, model, num_episodes, gamma, sigma)
#     all_episode_rewards.append(episode_rewards)
#     all_episode_steps.append(episode_steps)

# all_episode_rewards_array = np.array(all_episode_rewards)
# all_episode_steps_array = np.array(all_episode_steps)
# # print(all_episode_rewards)
# # print(all_episode_steps)

# all_episode_rewards_mean = np.mean(all_episode_rewards_array, axis=0)
# all_episode_steps_mean = np.mean(all_episode_steps_array, axis=0)

# all_episode_rewards_std = np.std(all_episode_rewards_array, axis=0)
# all_episode_steps_std = np.std(all_episode_steps_array, axis=0)

# plt.figure(figsize=(12, 6))
# plt.plot(np.arange(1, num_episodes + 1), all_episode_rewards_mean, label='Average(Mean) Reward', color='blue')
# plt.fill_between(np.arange(1, num_episodes + 1),
#                  all_episode_rewards_mean - all_episode_rewards_std,
#                  all_episode_rewards_mean + all_episode_rewards_std,
#                  color='orange', alpha=0.5, label='Standard Deviation')
# plt.title('Average Episode Reward vs. Number of Episodes in CartPole', fontsize=14)
# plt.xlabel('Number of Episodes', fontsize=12)
# plt.ylabel('Average Reward per Episode', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()

# plt.figure(figsize=(12, 6))
# plt.plot(np.arange(1, num_episodes + 1), all_episode_steps_mean, label='Mean Steps', color='green')
# plt.fill_between(np.arange(1, num_episodes + 1),
#                  all_episode_steps_mean - all_episode_steps_std,
#                  all_episode_steps_mean + all_episode_steps_std,
#                  color='lightgreen', alpha=0.5, label='Standard Deviation')
# plt.title('Average Episode Steps vs. Number of Episodes in CartPole', fontsize=14)
# plt.xlabel('Number of Episodes', fontsize=12)
# plt.ylabel('Average Steps per Episode', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()


# plt.figure(figsize=(12, 6))
# total_actions_taken = np.cumsum(all_episode_steps_mean)
# plt.plot(total_actions_taken, np.arange(1, num_episodes + 1), label='Learning Curve', color='green')
# plt.title('Cumulative Actions Taken vs. Number of Episodes in CartPole', fontsize=14)
# plt.xlabel('Cumulative Actions Taken', fontsize=12)
# plt.ylabel('Number of Episodes', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()
# # env.close()

# env_name = 'CartPole-v1'
# # env.make(env_name)

# all_episode_rewards = []
# all_episode_steps = []

# gamma = 0.925
# alpha_theta = 0.0003
# alpha_w = 0.0003

# fourier_order = 3

# sigma = 1.0

# RUNS = 5
# num_episodes = 1000

# for run in range(RUNS):
#     print(f"Run: {run}")
#     env = gym.make(env_name)
#     # env.reset()

#     input_size = env.observation_space.shape[0]
#     hidden_size = 64
#     output_size = env.action_space.n

#     model = ActorCriticCartpole(input_size, hidden_size, output_size)
#     policy_net = PolicyNetwork(input_size, hidden_size, output_size)
#     state_value_net = StateValueNetwork(input_size, hidden_size)

#     policy_net.eval()
#     state_value_net.eval()

#     #Parameters lr
#     optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
#     policy_optimizer = torch.optim.Adam(policy_net.parameters(), lr=0.001)
#     state_value_optimizer = torch.optim.Adam(state_value_net.parameters(), lr=0.001)

#     # print(model)
#     # print(policy_net)
#     # print(state_value_net)

#     policy_net, state_value_net, episode_rewards, episode_steps = actor_critic_function(policy_net, state_value_net, policy_optimizer, state_value_optimizer, env, model, num_episodes, gamma, sigma)
#     all_episode_rewards.append(episode_rewards)
#     all_episode_steps.append(episode_steps)

# all_episode_rewards_array = np.array(all_episode_rewards)
# all_episode_steps_array = np.array(all_episode_steps)
# # print(all_episode_rewards)
# # print(all_episode_steps)

# all_episode_rewards_mean = np.mean(all_episode_rewards_array, axis=0)
# all_episode_steps_mean = np.mean(all_episode_steps_array, axis=0)

# all_episode_rewards_std = np.std(all_episode_rewards_array, axis=0)
# all_episode_steps_std = np.std(all_episode_steps_array, axis=0)

# plt.figure(figsize=(12, 6))
# plt.plot(np.arange(1, num_episodes + 1), all_episode_rewards_mean, label='Average(Mean) Reward', color='blue')
# plt.fill_between(np.arange(1, num_episodes + 1),
#                  all_episode_rewards_mean - all_episode_rewards_std,
#                  all_episode_rewards_mean + all_episode_rewards_std,
#                  color='orange', alpha=0.5, label='Standard Deviation')
# plt.title('Average Episode Reward vs. Number of Episodes in CartPole', fontsize=14)
# plt.xlabel('Number of Episodes', fontsize=12)
# plt.ylabel('Average Reward per Episode', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()

# plt.figure(figsize=(12, 6))
# plt.plot(np.arange(1, num_episodes + 1), all_episode_steps_mean, label='Mean Steps', color='green')
# plt.fill_between(np.arange(1, num_episodes + 1),
#                  all_episode_steps_mean - all_episode_steps_std,
#                  all_episode_steps_mean + all_episode_steps_std,
#                  color='lightgreen', alpha=0.5, label='Standard Deviation')
# plt.title('Average Episode Steps vs. Number of Episodes in CartPole', fontsize=14)
# plt.xlabel('Number of Episodes', fontsize=12)
# plt.ylabel('Average Steps per Episode', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()


# plt.figure(figsize=(12, 6))
# total_actions_taken = np.cumsum(all_episode_steps_mean)
# plt.plot(total_actions_taken, np.arange(1, num_episodes + 1), label='Learning Curve', color='green')
# plt.title('Cumulative Actions Taken vs. Number of Episodes in CartPole', fontsize=14)
# plt.xlabel('Cumulative Actions Taken', fontsize=12)
# plt.ylabel('Number of Episodes', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()
# # env.close()

# env_name = 'CartPole-v1'
# # env.make(env_name)

# all_episode_rewards = []
# all_episode_steps = []

# gamma = 0.925
# alpha_theta = 0.001
# alpha_w = 0.001

# fourier_order = 3

# sigma = 1.0

# RUNS = 5
# num_episodes = 1000

# for run in range(RUNS):
#     print(f"Run: {run}")
#     env = gym.make(env_name)
#     # env.reset()

#     input_size = env.observation_space.shape[0]
#     hidden_size = 128
#     output_size = env.action_space.n

#     model = ActorCriticCartpole(input_size, hidden_size, output_size)
#     policy_net = PolicyNetwork(input_size, hidden_size, output_size)
#     state_value_net = StateValueNetwork(input_size, hidden_size)

#     policy_net.eval()
#     state_value_net.eval()

#     #Parameters lr
#     optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
#     policy_optimizer = torch.optim.Adam(policy_net.parameters(), lr=0.001)
#     state_value_optimizer = torch.optim.Adam(state_value_net.parameters(), lr=0.001)

#     # print(model)
#     # print(policy_net)
#     # print(state_value_net)

#     policy_net, state_value_net, episode_rewards, episode_steps = actor_critic_function(policy_net, state_value_net, policy_optimizer, state_value_optimizer, env, model, num_episodes, gamma, sigma)
#     all_episode_rewards.append(episode_rewards)
#     all_episode_steps.append(episode_steps)

# all_episode_rewards_array = np.array(all_episode_rewards)
# all_episode_steps_array = np.array(all_episode_steps)
# # print(all_episode_rewards)
# # print(all_episode_steps)

# all_episode_rewards_mean = np.mean(all_episode_rewards_array, axis=0)
# all_episode_steps_mean = np.mean(all_episode_steps_array, axis=0)

# all_episode_rewards_std = np.std(all_episode_rewards_array, axis=0)
# all_episode_steps_std = np.std(all_episode_steps_array, axis=0)

# plt.figure(figsize=(12, 6))
# plt.plot(np.arange(1, num_episodes + 1), all_episode_rewards_mean, label='Average(Mean) Reward', color='blue')
# plt.fill_between(np.arange(1, num_episodes + 1),
#                  all_episode_rewards_mean - all_episode_rewards_std,
#                  all_episode_rewards_mean + all_episode_rewards_std,
#                  color='orange', alpha=0.5, label='Standard Deviation')
# plt.title('Average Episode Reward vs. Number of Episodes in CartPole', fontsize=14)
# plt.xlabel('Number of Episodes', fontsize=12)
# plt.ylabel('Average Reward per Episode', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()

# plt.figure(figsize=(12, 6))
# plt.plot(np.arange(1, num_episodes + 1), all_episode_steps_mean, label='Mean Steps', color='green')
# plt.fill_between(np.arange(1, num_episodes + 1),
#                  all_episode_steps_mean - all_episode_steps_std,
#                  all_episode_steps_mean + all_episode_steps_std,
#                  color='lightgreen', alpha=0.5, label='Standard Deviation')
# plt.title('Average Episode Steps vs. Number of Episodes in CartPole', fontsize=14)
# plt.xlabel('Number of Episodes', fontsize=12)
# plt.ylabel('Average Steps per Episode', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()


# plt.figure(figsize=(12, 6))
# total_actions_taken = np.cumsum(all_episode_steps_mean)
# plt.plot(total_actions_taken, np.arange(1, num_episodes + 1), label='Learning Curve', color='green')
# plt.title('Cumulative Actions Taken vs. Number of Episodes in CartPole', fontsize=14)
# plt.xlabel('Cumulative Actions Taken', fontsize=12)
# plt.ylabel('Number of Episodes', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()
# # env.close()

# env_name = 'CartPole-v1'
# # env.make(env_name)

# all_episode_rewards = []
# all_episode_steps = []

# gamma = 0.925
# alpha_theta = 0.001
# alpha_w = 0.001

# fourier_order = 3

# sigma = 1.0

# RUNS = 5
# num_episodes = 1000

# for run in range(RUNS):
#     print(f"Run: {run}")
#     env = gym.make(env_name)
#     # env.reset()

#     input_size = env.observation_space.shape[0]
#     hidden_size = 128
#     output_size = env.action_space.n

#     model = ActorCriticCartpole(input_size, hidden_size, output_size)
#     policy_net = PolicyNetwork(input_size, hidden_size, output_size)
#     state_value_net = StateValueNetwork(input_size, hidden_size)

#     policy_net.eval()
#     state_value_net.eval()

#     #Parameters lr
#     optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
#     policy_optimizer = torch.optim.Adam(policy_net.parameters(), lr=0.001)
#     state_value_optimizer = torch.optim.Adam(state_value_net.parameters(), lr=0.001)

#     # print(model)
#     # print(policy_net)
#     # print(state_value_net)

#     policy_net, state_value_net, episode_rewards, episode_steps = actor_critic_function(policy_net, state_value_net, policy_optimizer, state_value_optimizer, env, model, num_episodes, gamma, sigma)
#     all_episode_rewards.append(episode_rewards)
#     all_episode_steps.append(episode_steps)

# all_episode_rewards_array = np.array(all_episode_rewards)
# all_episode_steps_array = np.array(all_episode_steps)
# # print(all_episode_rewards)
# # print(all_episode_steps)

# all_episode_rewards_mean = np.mean(all_episode_rewards_array, axis=0)
# all_episode_steps_mean = np.mean(all_episode_steps_array, axis=0)

# all_episode_rewards_std = np.std(all_episode_rewards_array, axis=0)
# all_episode_steps_std = np.std(all_episode_steps_array, axis=0)

# plt.figure(figsize=(12, 6))
# plt.plot(np.arange(1, num_episodes + 1), all_episode_rewards_mean, label='Average(Mean) Reward', color='blue')
# plt.fill_between(np.arange(1, num_episodes + 1),
#                  all_episode_rewards_mean - all_episode_rewards_std,
#                  all_episode_rewards_mean + all_episode_rewards_std,
#                  color='orange', alpha=0.5, label='Standard Deviation')
# plt.title('Average Episode Reward vs. Number of Episodes in CartPole', fontsize=14)
# plt.xlabel('Number of Episodes', fontsize=12)
# plt.ylabel('Average Reward per Episode', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()

# plt.figure(figsize=(12, 6))
# plt.plot(np.arange(1, num_episodes + 1), all_episode_steps_mean, label='Mean Steps', color='green')
# plt.fill_between(np.arange(1, num_episodes + 1),
#                  all_episode_steps_mean - all_episode_steps_std,
#                  all_episode_steps_mean + all_episode_steps_std,
#                  color='lightgreen', alpha=0.5, label='Standard Deviation')
# plt.title('Average Episode Steps vs. Number of Episodes in CartPole', fontsize=14)
# plt.xlabel('Number of Episodes', fontsize=12)
# plt.ylabel('Average Steps per Episode', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()


# plt.figure(figsize=(12, 6))
# total_actions_taken = np.cumsum(all_episode_steps_mean)
# plt.plot(total_actions_taken, np.arange(1, num_episodes + 1), label='Learning Curve', color='green')
# plt.title('Cumulative Actions Taken vs. Number of Episodes in CartPole', fontsize=14)
# plt.xlabel('Cumulative Actions Taken', fontsize=12)
# plt.ylabel('Number of Episodes', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()
# # env.close()

# env_name = 'CartPole-v1'
# # env.make(env_name)

# all_episode_rewards = []
# all_episode_steps = []

# gamma = 0.925
# alpha_theta = 0.0008
# alpha_w = 0.0008

# fourier_order = 3

# sigma = 1.0

# RUNS = 5
# num_episodes = 1000

# for run in range(RUNS):
#     print(f"Run: {run}")
#     env = gym.make(env_name)
#     # env.reset()

#     input_size = env.observation_space.shape[0]
#     hidden_size = 128
#     output_size = env.action_space.n

#     model = ActorCriticCartpole(input_size, hidden_size, output_size)
#     policy_net = PolicyNetwork(input_size, hidden_size, output_size)
#     state_value_net = StateValueNetwork(input_size, hidden_size)

#     policy_net.eval()
#     state_value_net.eval()

#     #Parameters lr
#     optimizer = torch.optim.Adam(model.parameters(), lr=0.0008)
#     policy_optimizer = torch.optim.Adam(policy_net.parameters(), lr=0.0008)
#     state_value_optimizer = torch.optim.Adam(state_value_net.parameters(), lr=0.0008)

#     # print(model)
#     # print(policy_net)
#     # print(state_value_net)

#     policy_net, state_value_net, episode_rewards, episode_steps = actor_critic_function(policy_net, state_value_net, policy_optimizer, state_value_optimizer, env, model, num_episodes, gamma, sigma)
#     all_episode_rewards.append(episode_rewards)
#     all_episode_steps.append(episode_steps)

# all_episode_rewards_array = np.array(all_episode_rewards)
# all_episode_steps_array = np.array(all_episode_steps)
# # print(all_episode_rewards)
# # print(all_episode_steps)

# all_episode_rewards_mean = np.mean(all_episode_rewards_array, axis=0)
# all_episode_steps_mean = np.mean(all_episode_steps_array, axis=0)

# all_episode_rewards_std = np.std(all_episode_rewards_array, axis=0)
# all_episode_steps_std = np.std(all_episode_steps_array, axis=0)

# plt.plot(np.arange(1, num_episodes + 1), all_episode_rewards_mean, label='Mean Reward', color='blue')
# plt.fill_between(np.arange(1, num_episodes + 1),
#                  all_episode_rewards_mean - all_episode_rewards_std,
#                  all_episode_rewards_mean + all_episode_rewards_std,
#                  color='lightblue', alpha=0.5, label='Standard Deviation')
# plt.xlabel('Episode')
# plt.ylabel('Total Reward')
# plt.title(f'Average Rewards over {RUNS} Runs - {env_name}')
# plt.legend()
# plt.show()

# plt.plot(np.arange(1, num_episodes + 1), all_episode_steps_mean, label='Mean Steps', color='green')
# plt.fill_between(np.arange(1, num_episodes + 1),
#                  all_episode_steps_mean - all_episode_steps_std,
#                  all_episode_steps_mean + all_episode_steps_std,
#                  color='lightgreen', alpha=0.5, label='Standard Deviation')
# plt.xlabel('Episode')
# plt.ylabel('Steps')
# plt.title(f'Average Rewards over {RUNS} Runs - {env_name}')
# plt.legend()
# plt.show()


# plt.figure(figsize=(12, 6))
# total_actions_taken = np.cumsum(all_episode_steps_mean)
# plt.plot(total_actions_taken, np.arange(1, num_episodes + 1), label='Cumulative Total Actions', color='orange')
# plt.fill_between(np.arange(1, num_episodes + 1),
#                  all_episode_steps_mean - all_episode_steps_std,
#                  all_episode_steps_mean + all_episode_steps_std,
#                  color='lightcoral', alpha=0.5, label='Standard Deviation')
# plt.xlabel('Total Actions Taken')
# plt.ylabel('Episode')
# plt.title(f'Learning Curve: Total Actions Taken over {RUNS} Run(s) - {env_name}')
# plt.legend()
# plt.show()

# # env.close()

# env_name = 'CartPole-v1'
# # env.make(env_name)

# all_episode_rewards = []
# all_episode_steps = []

# gamma = 0.925
# alpha_theta = 0.0008
# alpha_w = 0.0008

# fourier_order = 3

# sigma = 1.0

# RUNS = 5
# num_episodes = 1000

# for run in range(RUNS):
#     print(f"Run: {run}")
#     env = gym.make(env_name)
#     # env.reset()

#     input_size = env.observation_space.shape[0]
#     hidden_size = 128
#     output_size = env.action_space.n

#     model = ActorCriticCartpole(input_size, hidden_size, output_size)
#     policy_net = PolicyNetwork(input_size, hidden_size, output_size)
#     state_value_net = StateValueNetwork(input_size, hidden_size)

#     policy_net.eval()
#     state_value_net.eval()

#     #Parameters lr
#     optimizer = torch.optim.Adam(model.parameters(), lr=0.0008)
#     policy_optimizer = torch.optim.Adam(policy_net.parameters(), lr=0.0008)
#     state_value_optimizer = torch.optim.Adam(state_value_net.parameters(), lr=0.0008)

#     # print(model)
#     # print(policy_net)
#     # print(state_value_net)

#     policy_net, state_value_net, episode_rewards, episode_steps = actor_critic_function(policy_net, state_value_net, policy_optimizer, state_value_optimizer, env, model, num_episodes, gamma, sigma)
#     all_episode_rewards.append(episode_rewards)
#     all_episode_steps.append(episode_steps)

# all_episode_rewards_array = np.array(all_episode_rewards)
# all_episode_steps_array = np.array(all_episode_steps)
# # print(all_episode_rewards)
# # print(all_episode_steps)

# all_episode_rewards_mean = np.mean(all_episode_rewards_array, axis=0)
# all_episode_steps_mean = np.mean(all_episode_steps_array, axis=0)

# all_episode_rewards_std = np.std(all_episode_rewards_array, axis=0)
# all_episode_steps_std = np.std(all_episode_steps_array, axis=0)

# plt.figure(figsize=(12, 6))
# plt.plot(np.arange(1, num_episodes + 1), all_episode_rewards_mean, label='Average(Mean) Reward', color='blue')
# plt.fill_between(np.arange(1, num_episodes + 1),
#                  all_episode_rewards_mean - all_episode_rewards_std,
#                  all_episode_rewards_mean + all_episode_rewards_std,
#                  color='orange', alpha=0.5, label='Standard Deviation')
# plt.title('Average Episode Reward vs. Number of Episodes in CartPole', fontsize=14)
# plt.xlabel('Number of Episodes', fontsize=12)
# plt.ylabel('Average Reward per Episode', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()

# plt.figure(figsize=(12, 6))
# plt.plot(np.arange(1, num_episodes + 1), all_episode_steps_mean, label='Mean Steps', color='green')
# plt.fill_between(np.arange(1, num_episodes + 1),
#                  all_episode_steps_mean - all_episode_steps_std,
#                  all_episode_steps_mean + all_episode_steps_std,
#                  color='lightgreen', alpha=0.5, label='Standard Deviation')
# plt.title('Average Episode Steps vs. Number of Episodes in CartPole', fontsize=14)
# plt.xlabel('Number of Episodes', fontsize=12)
# plt.ylabel('Average Steps per Episode', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()


# plt.figure(figsize=(12, 6))
# total_actions_taken = np.cumsum(all_episode_steps_mean)
# plt.plot(total_actions_taken, np.arange(1, num_episodes + 1), label='Learning Curve', color='green')
# plt.title('Cumulative Actions Taken vs. Number of Episodes in CartPole', fontsize=14)
# plt.xlabel('Cumulative Actions Taken', fontsize=12)
# plt.ylabel('Number of Episodes', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()

# env_name = 'CartPole-v1'
# # env.make(env_name)

# all_episode_rewards = []
# all_episode_steps = []

# gamma = 0.925
# alpha_theta = 0.01
# alpha_w = 0.01

# fourier_order = 3

# sigma = 1.0

# RUNS = 5
# num_episodes = 1000

# for run in range(RUNS):
#     print(f"Run: {run}")
#     env = gym.make(env_name)
#     # env.reset()

#     input_size = env.observation_space.shape[0]
#     hidden_size = 128
#     output_size = env.action_space.n

#     model = ActorCriticCartpole(input_size, hidden_size, output_size)
#     policy_net = PolicyNetwork(input_size, hidden_size, output_size)
#     state_value_net = StateValueNetwork(input_size, hidden_size)

#     policy_net.eval()
#     state_value_net.eval()

#     #Parameters lr
#     optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
#     policy_optimizer = torch.optim.Adam(policy_net.parameters(), lr=0.001)
#     state_value_optimizer = torch.optim.Adam(state_value_net.parameters(), lr=0.001)

#     # print(model)
#     # print(policy_net)
#     # print(state_value_net)

#     policy_net, state_value_net, episode_rewards, episode_steps = actor_critic_function(policy_net, state_value_net, policy_optimizer, state_value_optimizer, env, model, num_episodes, gamma, sigma)
#     all_episode_rewards.append(episode_rewards)
#     all_episode_steps.append(episode_steps)

# all_episode_rewards_array = np.array(all_episode_rewards)
# all_episode_steps_array = np.array(all_episode_steps)
# # print(all_episode_rewards)
# # print(all_episode_steps)

# all_episode_rewards_mean = np.mean(all_episode_rewards_array, axis=0)
# all_episode_steps_mean = np.mean(all_episode_steps_array, axis=0)

# all_episode_rewards_std = np.std(all_episode_rewards_array, axis=0)
# all_episode_steps_std = np.std(all_episode_steps_array, axis=0)

# plt.figure(figsize=(12, 6))
# plt.plot(np.arange(1, num_episodes + 1), all_episode_rewards_mean, label='Average(Mean) Reward', color='blue')
# plt.fill_between(np.arange(1, num_episodes + 1),
#                  all_episode_rewards_mean - all_episode_rewards_std,
#                  all_episode_rewards_mean + all_episode_rewards_std,
#                  color='orange', alpha=0.5, label='Standard Deviation')
# plt.title('Average Episode Reward vs. Number of Episodes in CartPole', fontsize=14)
# plt.xlabel('Number of Episodes', fontsize=12)
# plt.ylabel('Average Reward per Episode', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()

# plt.figure(figsize=(12, 6))
# plt.plot(np.arange(1, num_episodes + 1), all_episode_steps_mean, label='Mean Steps', color='green')
# plt.fill_between(np.arange(1, num_episodes + 1),
#                  all_episode_steps_mean - all_episode_steps_std,
#                  all_episode_steps_mean + all_episode_steps_std,
#                  color='lightgreen', alpha=0.5, label='Standard Deviation')
# plt.title('Average Episode Steps vs. Number of Episodes in CartPole', fontsize=14)
# plt.xlabel('Number of Episodes', fontsize=12)
# plt.ylabel('Average Steps per Episode', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()


# plt.figure(figsize=(12, 6))
# total_actions_taken = np.cumsum(all_episode_steps_mean)
# plt.plot(total_actions_taken, np.arange(1, num_episodes + 1), label='Learning Curve', color='green')
# plt.title('Cumulative Actions Taken vs. Number of Episodes in CartPole', fontsize=14)
# plt.xlabel('Cumulative Actions Taken', fontsize=12)
# plt.ylabel('Number of Episodes', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()
# # env.close()

# """# **Acrobot**"""

# actions = [0, 1, 2]

# def action_selection(state, actor_critic_model, sigma):
#     state = torch.tensor(state, dtype=torch.float32)
#     state = state.unsqueeze(0)
#     actor_policy, actor_state_value = actor_critic_model(state)
#     state_exploration = sigma * actor_policy
#     action_probs = torch.softmax(state_exploration, dim=-1)
#     # action_probs = actor_policy.detach().numpy()[0]
#     # action_probs = np.exp(action_probs) / np.sum(np.exp(action_probs))
#     action_probs_dist = Categorical(action_probs)
#     dist = torch.distributions.Categorical(logits=action_probs)
#     action = dist.sample()
#     # action = np.random.choice(len(action_probs), p=action_probs)
#     return action.item(), dist.log_prob(action), action_probs

# class ActorCriticAcrobot(nn.Module):
#     def __init__(self, input_size, hidden_size, output_size, fourier_order):
#         super(ActorCriticAcrobot, self).__init__()
#         self.actor_policy_network = PolicyNetwork(input_size * fourier_order, hidden_size, output_size)
#         self.actor_state_value_network = StateValueNetwork(input_size * fourier_order, hidden_size)
#         self.fourier_features = FourierFeatures(input_size, fourier_order)

#         self.statemean = torch.zeros(input_size)
#         self.statestd = torch.ones(input_size)
#         self.actionmean = torch.zeros(output_size)
#         self.actionstd = torch.ones(output_size)
#         self.action_space = output_size
#         self.state_space = input_size
#         self.hidden_size = hidden_size
#         self.fourier_order = fourier_order

#     def normalize_state(self, state):
#         min_max_val = [
#             [-1, 1],
#             [-1, 1],
#             [-1, 1],
#             [-1, 1],
#             [-12.567, 12.567],
#             [-28.274, 28.274]
#         ]
#         normalized_state = []
#         for i in range(len(state)):
#             normalized_state.append((state[i] - min_max_val[i][0]) / (min_max_val[i][1] - min_max_val[i][0]))

#         normalized_state = torch.stack(normalized_state)
#         return normalized_state

#     # def normalize_state(self, state):
#     #     return (state - self.statemean) / self.statestd

#     def normalize_action(self, action):
#         return (action - self.actionmean) / self.actionstd

#     def forward(self, x):
#         state_features = x
#         state_features = self.normalize_state(state_features)
#         features = self.fourier_features(state_features)
#         actor_policy = self.actor_policy_network(features)
#         actor_state_value = self.actor_state_value_network(features)
#         return actor_policy, actor_state_value

# def update_networks(policy_optimizer, actor_policy_loss, state_value_optimizer, critic_state_value_loss):

#     policy_optimizer.zero_grad()
#     # for param in policy_net.parameters():
#     #     param.grad = None
#     actor_policy_loss.backward()
#     policy_optimizer.step()

#     state_value_optimizer.zero_grad()
#     # for param in state_value_net.parameters():
#     #     param.grad = None
#     critic_state_value_loss.backward()
#     state_value_optimizer.step()

# def actor_critic_function(policy_net, state_value_net, policy_optimizer, state_value_optimizer, env, model, num_episodes, gamma, sigma):

#     all_episode_rewards = []
#     all_episode_steps = []

#     for episode in range(num_episodes):
#         state = env.reset()
#         state = torch.tensor(state, dtype=torch.float32)
#         done = False
#         episode_states = []
#         episode_actions = []
#         episode_rewards = []
#         episode_values = []
#         episode_reward = 0
#         episode_steps = 0

#         while not done:
#             action, dist_log_prob, action_probs = action_selection(state, model, sigma)

#             step_result = env.step(action)
#             next_state = step_result[0]
#             reward = step_result[1]
#             done = step_result[2]
#             timeout = step_result[3]

#             state_value = state_value_net(state)
#             next_state = torch.tensor(next_state, dtype=torch.float32)
#             next_state_value = state_value_net(next_state)

#             current_cumulative_reward = reward + gamma * next_state_value - state_value
#             # current_cumulative_reward = current_cumulative_reward - state_value
#             action_prob = dist_log_prob.exp()

#             # action_index = actions.index(action)
#             # action_prob = action_prob[action_index]

#             detach_current_cumulative_reward = current_cumulative_reward.detach()
#             actor_policy_loss = -dist_log_prob * detach_current_cumulative_reward
#             critic_state_value_loss = current_cumulative_reward.pow(2)

#             update_networks(policy_optimizer, actor_policy_loss, state_value_optimizer, critic_state_value_loss)

#             episode_states.append(state)
#             episode_actions.append(action)
#             episode_rewards.append(reward)
#             episode_values.append(dist_log_prob)

#             state = next_state
#             state = torch.tensor(state, dtype=torch.float32)
#             episode_reward += reward
#             episode_steps += 1

#         if episode % 100 == 0:
#             print(f"Episode {episode}/{num_episodes}, Total Reward: {episode_reward}")

#         all_episode_rewards.append(episode_reward)
#         all_episode_steps.append(episode_steps)

#     return policy_net, state_value_net, all_episode_rewards, all_episode_steps

# env_name = 'Acrobot-v1'
# # env.make(env_name)

# all_episode_rewards = []
# all_episode_steps = []

# gamma = 0.925
# alpha_theta = 0.0001
# alpha_w = 0.0001

# fourier_order = 3

# sigma = 1.0

# RUNS = 5
# num_episodes = 1000

# for run in range(RUNS):
#     print(f"Run: {run}")
#     env = gym.make(env_name)
#     # env.reset()

#     input_size = env.observation_space.shape[0]
#     hidden_size = 128
#     output_size = env.action_space.n

#     model = ActorCriticAcrobot(input_size, hidden_size, output_size, fourier_order)
#     policy_net = PolicyNetwork(input_size, hidden_size, output_size)
#     state_value_net = StateValueNetwork(input_size, hidden_size)

#     policy_net.eval()
#     state_value_net.eval()

#     #Parameters lr
#     optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
#     policy_optimizer = torch.optim.Adam(policy_net.parameters(), lr=0.001)
#     state_value_optimizer = torch.optim.Adam(state_value_net.parameters(), lr=0.001)

#     # print(model)
#     # print(policy_net)
#     # print(state_value_net)

#     policy_net, state_value_net, episode_rewards, episode_steps = actor_critic_function(policy_net, state_value_net, policy_optimizer, state_value_optimizer, env, model, num_episodes, gamma, sigma)
#     all_episode_rewards.append(episode_rewards)
#     all_episode_steps.append(episode_steps)

# all_episode_rewards_array = np.array(all_episode_rewards)
# all_episode_steps_array = np.array(all_episode_steps)
# # print(all_episode_rewards)
# # print(all_episode_steps)

# all_episode_rewards_mean = np.mean(all_episode_rewards_array, axis=0)
# all_episode_steps_mean = np.mean(all_episode_steps_array, axis=0)

# all_episode_rewards_std = np.std(all_episode_rewards_array, axis=0)
# all_episode_steps_std = np.std(all_episode_steps_array, axis=0)

# plt.figure(figsize=(12, 6))
# plt.plot(np.arange(1, num_episodes + 1), all_episode_rewards_mean, label='Average(Mean) Reward', color='blue')
# plt.fill_between(np.arange(1, num_episodes + 1),
#                  all_episode_rewards_mean - all_episode_rewards_std,
#                  all_episode_rewards_mean + all_episode_rewards_std,
#                  color='orange', alpha=0.5, label='Standard Deviation')
# plt.title('Average Episode Reward vs. Number of Episodes in Acrobot', fontsize=14)
# plt.xlabel('Number of Episodes', fontsize=12)
# plt.ylabel('Average Reward per Episode', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()

# plt.figure(figsize=(12, 6))
# plt.plot(np.arange(1, num_episodes + 1), all_episode_steps_mean, label='Average Steps', color='green')
# plt.fill_between(np.arange(1, num_episodes + 1),
#                  all_episode_steps_mean - all_episode_steps_std,
#                  all_episode_steps_mean + all_episode_steps_std,
#                  color='lightgreen', alpha=0.5, label='Standard Deviation')
# plt.title('Average Episode Steps vs. Number of Episodes in Acrobot', fontsize=14)
# plt.xlabel('Number of Episodes', fontsize=12)
# plt.ylabel('Average Steps per Episode', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()


# plt.figure(figsize=(12, 6))
# total_actions_taken = np.cumsum(all_episode_steps_mean)
# plt.plot(total_actions_taken, np.arange(1, num_episodes + 1), label='Learning Curve', color='green')
# plt.title('Cumulative Actions Taken vs. Number of Episodes in Acrobot', fontsize=14)
# plt.xlabel('Cumulative Actions Taken', fontsize=12)
# plt.ylabel('Number of Episodes', fontsize=12)
# plt.legend(fontsize=10)
# plt.show()
# # env.close()



